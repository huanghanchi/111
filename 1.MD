**Multi-agent Reinforcement Learning for Mention Recommendation**

ANONYMOUS AUTHOR(S)

Manuscript submitted to ACM

In social networking platforms such as Twitter, users often mention other related users (indicated by the “@” symbol) in their posts. Mention recommendation is the task of automatically suggesting candidate users who are likely to be mentioned, which can improve communication efficiency and the user experience. Existing work typically relies on recent posts or a few randomly selected historical messages to infer user preferences and relationships. However, these methods ignore two fundamental properties of mention recommendation, namely the dynamic cooperation among users and the sparsity of their interactions. To address these issues, we propose MA-DGNN, a graph-based multi-agent reinforcement learning (MARL) approach that consists of two key components: a time-delayed aggregation graph policy network (TAGPN) and a custom prioritized experience replay (PER). TAGPN applies a time-delayed graph support operation to act as a distributed controller for large networks of users with interaction dynamics and sparse communication. This operation enables a time-delayed multi-hop information diffusion mechanism that helps sparsely interacting users to obtain advice from densely interacting users. PER combines six carefully chosen metrics to handle sample imbalance and non-stationarity of user preferences and to leverage statistical patterns in user interactions. Moreover, MA-DGNN updates the user-user utility matrix by decomposing the neural matrix based on similarity loss to aggregate the output of TAGPN and aid in credit assignment in MARL. The loss functions of MA-DGNN’s policy and critic networks are modeled as minimal maximum objectives to learn robust policies in the face of multiple non-stationarity from both the environment and other agents. Extensive experiments on synthetic and real datasets demonstrate that our MA-GDNN outperforms the existing state-of-the-art approaches.

CCS Concepts: • Computing methodologies → Artificialintelligence.

Additional Key Words and Phrases: Mention Recommendation, Multi-agent Reinforcement Learning, Graph Neural Network

**1 INTRODUCTION**

Mention notificationis a very popular tool when people communicate on social networks. By tagging an ’@’ symbol with a username concatenated in the post message, a ’You have been mentioned’ notification would be pushed to the specific user. Automatically suggesting who is the best to be mentioned with an ’@’ is a key problem in social networks titled as mention recommendation. It brings in many benefits including but not limited to (i) strengthening the relationship with friends in that the one mentioned feels a special greeting and care from the user who publishes the tweet [[30](#_page18_x73.44_y493.55)]. The reactions make friends closely connected and spend more time and attention on each other. (ii) bridging over different circles, since the issued post could be forwarded to friends of friends more than we can imagine. It is similar to a view from [[16\]](#_page18_x73.44_y224.56) that the tagging activity promotes the crossover of social graphs on Twitter, which is widely applied in commercial promotion. Breaking the boundaries and touching the potential customers are just what the retailers want most; and (iii) improving reciprocity over a social network. Take LinkedIn for instance, the published post could be kept hot on the LinkedIn platform if the publisher mentions the right persons with great interest as well as great public impact. Similar examples on Facebook could be found in [\[12,](#_page18_x73.44_y144.85)[ 30\].](#_page18_x73.44_y493.55)

The whole process of mention recommendation is displayed in Figure [1. ](#_page1_x110.16_y89.56)For each recommendation round, in the beginning, a user colored pink issues a post and intends to ’@’ somebody. When the user inputs the ’@’ symbol, the algorithm (named MA-DGNN in Figure[ 1)](#_page1_x110.16_y89.56) should quickly recommend a list of candidate mentionees (with colors blue, green, yellow, and purple, etc.) for reference. Since the pink user has the biggest preference for the blue user, the pink user mentions the blue user ultimately. After receiving the ’mention’ notificationfrom the pink user, the blue user finds the post from the pink user very interesting and continues to share the post with other friends according to his social

1
Multi-agent Reinforcement Learning for Mention Recommendation

![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.001.png)

Fig. 1. Mention recommendation processed by our proposed MA-DGNN.

network. Therefore, by the information diffusion on a social network, the relationship between users is continuously evolving, which exerts influenceon not only the algorithm’s follow-up recommendation but the subsequent users’ ’@’ decision. Since mention recommendation is a multi-round recommendation process involving the complex and dynamic relationship among the post publisher, the post’s content, and other users on social networks, it seems more natural to use reinforcement learning (RL) instead of supervised learning which assumes environments’ stationarity, and to use multiple agents instead of a single agent to model evolving interactions between users while making personalized decisions for each user.

Existing studies on mention recommendation could be mainly categorized into three methods: ranking-based meth- ods [\[19,](#_page18_x73.44_y284.33)[ 35,](#_page18_x73.44_y553.32)[ 37,](#_page18_x73.44_y583.21)[ 48](#_page19_x110.16_y254.44)], classification-based methods [[7,](#_page17_x113.65_y590.50) [18\]](#_page18_x73.44_y264.41) and Multi-agent Reinforcement Learning (MARL) based methods [\[11](#_page18_x73.44_y114.97)[\].](#_page1_x112.99_y636.52)[^1] For ranking-based methods, the target user would be recommended with a ranked list of potential men- tionees achieved by a global scoring function. Similarly, classification-basedmethods address mention recommendation from a classification perspective, i.e., potential mentionees should be either predicted as positive or negative in each mention behavior towards the target user. However, these two methods fail to consider the following two issues. Firstly, the global scoring function or classification model, as a unified strategy for all users, could not well capture the very different styles of different users and their preferences evolving in different directions as time goes by. Secondly, the aforementioned methods generally suffer from the sparsity problem, i.e., lack of interaction signals between users. Ideally, different levels of relationships between different users and other related information should be fully leveraged to make better recommendation decisions for users with less historical interaction records.

To address these challenges, we propose MA-DGNN, a graph-based MARL model with deliberately customized prioritized experience replay to solve mention recommendation tasks with a large number of users. Each user on the social network would be treated as an agent and the interactions with other users would be well modeled as cooperations with the environment. The system is expected to automatically generate highly qualifiedmentionee candidates such that the![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.002.png)

target user would prefer to hit them with the symbol ’@’. First, during state embedding, we employ an LSTM-based graph neural network to model users’ complex time-varying dynamics, since the messages along with user interactions diffuse over the social network in a sequential manner. To handle a large number of users and the sparsity and non-stationarity of interactions between users, we adopt the delayed aggregation graph neural network (DGNN) [\[36\]](#_page18_x73.44_y563.29) as the policy network of MARL, which combines the time-delayed graph operation of different orders to serve as distributed controllers for large networks of users with interacting dynamics and sparsely available communications. Thanks to different levels of information diffusion along the multi-hop neighbors in DGNN, the relationship between nodes with sparse interactions might be well inferred from the flowing information among other nodes with varying distances. Last but not least, a customized personalized replay buffer (PER) with non-stationarity detection and state clustering techniques is also utilized to make full use of historical statistics and handle imbalanced datasets as well as sparse user interactions.

Below, we summarize our main contributions as four-fold:

- We are the firstones to model each user as an agent in mention recommendation problem and the firstone to introduce a graph-based MARL approach with time-delayed graph support which models the sparse and dynamic interactions among users to solve the problem with a large number of users. A customized PER mechanism and a reward reshaping technique are designed elaborately to handle the sample imbalance and the non-stationarity of users’ preferences, and take full advantage of the statistical laws inside users’ interactions.
- During the training of MARL, we customize minimax objectives to minimize the losses of the policy and critic networks for a worst-case scenario, so as to learn robust policies in the face of multiple non-stationarity from both environments and other agents. The minimax optimization technique, the customized PER mechanism, and the LSTM-based graph neural network for state embedding together empower our algorithm with great adaptivity to the changes of both tweets’ styles and users’ interactions.
- To make a comprehensive aggregation for the outputs of DGNN as well as performing credit assignment, we construct an agent-agent utility matrix which learns the contribution of each user’s suggestion for other users and regard each row of the matrix as the aggregation coefficients.The neural matrix factorization technique, which can well deal with data sparsity by learning hidden factors, is novelly proposed to optimize the matrix. An ablation study is performed to successfully show the great performance improvement when using this decision aggregation mechanism.
- Extensive experiments on the synthetic and real-world datasets with six classic evaluation metrics demonstrate the absolute performance superiority of our algorithm over existing methods.

**2 RELATED WORK**

1. **Mention Recommendation**

There are a bunch of studies proposed to address mention recommendation. The most popular employed methods are ranking-based models and classification-basedmodels. For ranking-based methods, a global ranking function could be achieved by (semi-) supervised learning. Specifically, Wang et al. [[37\]](#_page18_x73.44_y583.21) adopt ranking support vector regression with features respecting user interest match, user relationship, and user influence.Tang et al. [[35\]](#_page18_x73.44_y553.32) apply the ranking support vector machine with features regarding content, social, location, and time. Wang et al. [[38\]](#_page18_x73.44_y603.14) introduce a semi-supervised deep model to capture the highly non-linear relationship between nodes.

For classification-based models, whether to recommend each user is deduced by tackling several classification problems. Pramanik et al. [\[26\]](#_page18_x73.44_y423.81) introduce a tweet propagation model based on a multiplex network framework to handle the classification problems, which allows analyzing the effects of mentioning on finalretweet count. Wang et al. [[39\]](#_page18_x73.44_y623.06)

propose a generative model to solve mention recommendation by learning users’ semantic patterns and the correlation between users’ multi-modal mention documents in a unifiedway. Yi et al.[ \[42\]](#_page19_x110.16_y134.89) construct a heterogeneous mention network to model different entities such as authors, messages, and users into a unifiedlow-dimensional embedding vector space, such that the heterogeneous information can be calculated in the same embedding space.

However, the above approaches and the MARL method in [\[11\]](#_page18_x73.44_y114.97) take a unifiedstrategy to serve for all users which might neglect users’ personalized preferences. Besides, these methods do not fully characterize the two primary properties of mention recommendation, i.e., the dynamic and sparse communications of user behaviors, which make them not quite viable in our problem setting. In contrast, we model each user as an intelligence, providing each user with a customised recommendation strategy that draws on their own historical experience and the suggestions of others, while taking into account the dynamic and sparse nature of different user interactions, allowing the algorithm to provide high quality recommendations for each user despite the highly dynamic and limited amount of data.

2. **Reinforcement Learning for Recommendation**

Reinforcement learning (RL) is the process of agent learning to maximize the long-term rewards when interacting with the environment [[34](#_page18_x73.44_y543.36)]. It has been widely introduced into recommendation systems due to its consideration of users’ long-term feedback [\[1,](#_page17_x113.65_y490.88)[ 21](#_page18_x73.44_y324.18)]. Via interacting with a target user, the recommendation platform would learn her item preference step by step according to the reward, e.g., the user’s click or longer dwelling time. Existing RL methods for recommendation can be roughly divided into two categories, value-based methods [[8,](#_page17_x113.65_y610.43) [41,](#_page19_x110.16_y114.97) [44,](#_page19_x110.16_y174.74) [46,](#_page19_x110.16_y214.59) [47\]](#_page19_x110.16_y234.52) and policy-based methods [\[6,](#_page17_x113.65_y570.58)[ 13,](#_page18_x73.44_y164.78)[ 14,](#_page18_x73.44_y184.71)[ 22,](#_page18_x73.44_y344.11)[ 29,](#_page18_x73.44_y473.62)[ 31,](#_page18_x73.44_y513.47)[ 40,](#_page19_x110.16_y95.04)[ 43,](#_page19_x110.16_y154.82)[ 45\].](#_page19_x110.16_y194.67)

Value-based methods evaluate the Q value for each candidate item (or item set) and select the item (or item set) with the highest Q-value. Recent literature approximates Q-values mostly via Deep Q-Networks. For example, Zheng et al. [\[47\]](#_page19_x110.16_y234.52) propose a Deep Q-Learning based recommendation framework with the user return pattern as a supplement to user feedback. Similarly, Zhao et al. [\[46\]](#_page19_x110.16_y214.59) leverage the Deep Q-Network and integrate skipped items (negative feedback) into RL-based recommendation methods. Nevertheless, it can be computation inefficientfor value-based methods to evaluate Q-values for all items when the item space is very huge. As a consequence, some literature adopts policy-based methods. They explicitly construct a representation of the policy, which maps environment state to action and store it in memory during the learning process. For instance, Zhao et al. [\[45\]](#_page19_x110.16_y194.67) propose a page-wise recommendation framework that leverages Deep Deterministic Policy Gradient (DDPG) to automatically learn the optimal strategies. Meanwhile, Hu et al. [[14\] ](#_page18_x73.44_y184.71)develop a policy gradient algorithm to learn an optimal ranking policy, as well as dealing with the high reward variance and unbalanced reward distribution in the search session Markov decision process.

However, seldom do existing RL strategies consider tackling the mention recommendation problem, where the interactions between the target user with the others are affected by the complex time-varying dynamics and the interaction sparsity makes the learning process difficult to address. Although the proposed RL-based strategies share a similar long-term rewards maximization goal, they are actually not feasible for our problem[2.](#_page3_x113.40_y587.41)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.002.png)

2To achieve personalized mention recommendation and better utilization on users’ interactions, it seems promising to utilize MARL which could model the personalized strategy for each user as each individual agent and infer relationship among users by learning agents’ dynamic and cooperative behaviors. Although MARL-based methods could adopt personalized strategies and help alleviate the sparsity problem, they are still in face of specificlimitations to be overcome. Firstly, samples from MARL’s replay buffer might be out of date due to users’ dynamic preference, and the mixture of noisy samples from various users would cause a great interference to policy updating and inferring process. Secondly, when updating a policy for a certain agent, other agents could be regarded as environments and the non-stationarity of other agents (i.e., environments) breaks the Markov assumption of single-agent RL, which might result in performance degradation or even failures of policy learning. To handle these challenges, we equip our MA-DGNN with a series of novel techniques which demonstrate great performances in the experimental session.

Table 1. Notations.



|Notation||Meaning|
| - | :- | - |
|T||total number of rounds|
|t||current time|
|N||number of users|
|<p>𝑡𝑚𝑡</p><p>𝑜𝑡 𝑡ℎ𝑟𝑒𝑡</p><p>𝑥𝑗</p><p>𝑦𝑗</p><p>𝑝𝑡</p><p>𝑛𝑡 𝑟𝑡,𝑖</p><p>𝑆𝑛</p>||<p>message at time t</p><p>linearly weighted value output by DGNN and utility matrix</p><p>threshold which decides whether to recommend the most recent K users that user𝑖𝑡has mentioned or to select K users (except user𝑖𝑡) with the most similar state vectors to user 𝑖𝑡</p><p>the feature vector of the 𝑗-th latest tweet that user 𝑗has issued</p><p>the feature vector of the 𝑗-th latest user that user 𝑗has mentioned</p><p>the number of historical positive samples till time 𝑡</p><p>the number of historical negative samples till time 𝑡</p><p>the reward of agent 𝑖∈ [𝑁] at round 𝑡</p><p>a graph shift operator that describes the graph support𝐺𝑛</p>|
3. **Multi-agent Reinforcement Learning for Recommendation 3 METHODOLOGY**

In this part, we firstformulate the mention recommendation task in Section [3.1 ](#_page4_x73.44_y384.33)and then present the overview and core components of our proposed method MA-DGNN in Section[ 3.2.](#_page4_x73.44_y481.79)

1. **Problem Formulation**

Assume there are 𝑁users in a social network platform. At round 𝑡∈ [𝑇], user 𝑖𝑡comes into the system and issues a message 𝑡𝑚𝑡, e.g., a tweet or microblog. Given 𝑖𝑡and 𝑡𝑚𝑡, the system must recommend one or several users from a list of candidate mentioned users [[𝑁](#_page4_x76.68_y636.22)][  ](#_page4_x76.68_y636.22)[^2]according to their long-term historical tweets H . The objective is to have an overall great performance with regard to precision, recall, and F1-score, the three popular evaluation metrics in mention recommendation tasks and traditional recommendation system field.

2. **Algorithm**

Figure[ 2 ](#_page5_x110.16_y89.56)shows the architecture of our proposed method MA-DGNN. To model the dynamic relationship between users and make a personalized recommendation for each user, we regard each user as a node in a graph, as well as an agent in the multi-agent reinforcement learning (MARL) system. At round 𝑡∈ [𝑇], a delayed aggregation graph neural network (DGNN) is utilized to output a real value for each node/user, which applies the delayed graph support operation to serve as distributed controllers for large networks of users with interacting dynamics and sparsely available communications. Then we use the𝑖𝑡-th row of a user-user utility matrix to aggregate the real values for every user and output a linearly weighted value 𝑜𝑡∈ [0,1] to decide which users should be recommended. The utility matrix models the dynamic relationship between each pair of users and is updated by a neural matrix factorization method with the loss considering the similarity between users. It is optimized independently to DGNN instead of being jointly and thus slowly updated with DGNN

Mentioned Tweet![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.003.png) User i\_t history

**Round t SeqConv: GCN-LSTM**

**Static state Dynamic state**

**Decentralized actor Centralized critic ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.004.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.005.png)**

**User-user utility matrix** Composite loss ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.006.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.007.jpeg)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.008.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.009.png)

Update by neural matrix factorization: **Delayed Aggregation Credit Assignment![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.010.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.010.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.011.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.012.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.013.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.014.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.015.png)**

Agent 1 **GNN**

loss 1-scaled cross-entropy Agent N

Agent N-1 **Prioritized experience ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.016.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.010.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.010.png)**Agent 2 ...... **replay![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.017.png)**

loss 2-similarity-based term ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.018.png)

![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.019.png) ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.005.png) ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.008.png) Non-stationary detection![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.020.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.009.png)

Ultimate**＋** decision **User feedback![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.021.jpeg)**

Fig. 2. The overall framework of MA-DGNN.

via only one feedback at each round. After obtaining 𝑜𝑡, we compare it to a threshold 𝑡ℎ[𝑟](#_page5_x113.21_y628.22)𝑒[𝑡4.](#_page5_x113.21_y628.22) If 𝑜𝑡> 𝑡ℎ𝑟𝑒𝑡, then we recommend the most recent K users that user 𝑖𝑡has mentioned; otherwise, the K users (except user 𝑖𝑡) with the most similar state vectors to user 𝑖𝑡are selected for the recommendation to user 𝑖𝑡(the similary is defined by the cosine operation between two vectors).

We use a unified critic network and decentralized actor networks in MARL with minimax optimization for robust policy learning and adopt a customized experience replay mechanism with state clustering and non-stationarity detection to update both the policy network and the critic network, which consider both the sample diversity and users’ varying preferences. As for the critic loss, we perform credit assignment to the 𝑁agents with weights being the 𝑖𝑡-th row of the user-user utility matrix, which is proven to obviously improve the performance of our MA-DGNN in the ablation study part. Lastly, to handle the seriously imbalanced data, we scale rewards in different scenarios and the different terms in the loss of neural matrix factorization according to the relative ratio between positive samples and negative samples[5.](#_page5_x113.40_y636.97)

Below, we give a detailed description of each module.

1. The composition of Markov Decision Process. We model the long-term mention recommendation process as a Markov Decision Process (MDP) and introduce the state, action, and reward configurationin the MDP in detail. State. We regard each user as an agent in MARL. For𝑖∈ [𝑁], the state of agent𝑖consists of two parts: the static and dynamic parts.
- The static part is the pre-trained feature vector of user 𝑖, which shows the user’s inherent features such as birth, birthplace, education, etc.
- With regard to thedynamicpart, since the historically issued tweet list and the mentionee list of a user are essential to the user’s characterization, we also consider SeqConv [\[33](#_page18_x73.44_y533.40)], an LSTM-based graph convolutional operator, which is useful for graph datasets where each node represents a sequence, to deal with the two aforementioned time series and form the dynamic part of a user’s state. Specifically, the dynamic part of user (agent)𝑖’s state is 𝑆𝑒𝑞𝐶𝑜𝑛𝑣([𝑥1,𝑦1],[𝑥2,𝑦2],···,

4𝑡ℎ𝑟𝑒𝑡is a sliding-window threshold which will be described later.![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.022.png)

5Positive samples refer to samples in the rounds where the mentionee belongs to the most recentK users that the currently served user has mentioned. The negative samples are samples in the remaining rounds.

[𝑥𝑁 ,𝑦𝑁 ]), where 𝑥𝑗(𝑗∈ [𝑁𝑙𝑖𝑠𝑡]) refers to the feature vector of the 𝑗-th latest tweet that user 𝑖has issued, and

𝑙𝑖𝑠𝑡𝑙𝑖𝑠𝑡

𝑦𝑗(𝑗∈ [𝑁𝑙𝑖𝑠𝑡]) refers to the feature vector of the 𝑗-th latest user that user𝑖has mentioned. In the beginning, when the system has not collected enough samples, the uncollected 𝑥𝑗and𝑦𝑗are set to be the zero vectors.

The state input to the policy network and the critic network as stated below is the concatenation of all agents’ states. Action. The action is the concatenation of the 𝑁real values output by the graph-based policy network, as stated below. The ultimate decision, however, depends on whether 𝑜𝑡, the linear combination of the 𝑁real values, exceeds a

sliding-window threshold 𝑡ℎ𝑟𝑒𝑡, the 𝑛𝑡+1 percentile of the list {𝑜𝑖}𝑖∈[𝑡−1], where 𝑝𝑡is the number of historical positive

𝑝𝑡+1

samples and𝑛𝑡is the number of historical negative samples. If yes, then the ultimate decision is1, i.e., the algorithm will recommend the most recent K users that the currently served user has mentioned to that user. If not, then the ultimate decision is 0, i.e., the algorithm will select theK users with the most similar state vectors to the currently served user for

recommendation. As can be seen in the ablation study, setting 𝑡ℎ𝑟𝑒𝑡to be 𝑛𝑝𝑡++11, which considers datasets’ imbalanced

𝑡

property, instead of the normal threshold0.5 improves the performance of our MA-DGNN remarkably in all real datasets. Reward. To update each node of DGNN according to their different contributions for the ultimate decision, we

customize different rewards for each node at each round. Specifically, at each round 𝑡, our algorithm will output a real value𝑜0 ∈ [0,1] for each user𝑖∈ [𝑁] and we thus sample a binary value𝑜1 ∼𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖(𝑜0 ). If the mentioned user at

𝑡,𝑖 𝑡,𝑖 𝑡,𝑖

round𝑡was mentioned by user𝑖𝑡in user𝑖𝑡’s most recent K rounds, then we say that the behavior at round𝑡is a positive sample and the ground truth at round𝑡is 1; otherwise, the behavior at round𝑡is a negative sample and the ground truth is 0. Since in real datasets, the number of negative samples far outweighs the number of positive samples, we scale the terms in rewards to handle the issue of imbalanced datasets. Specifically, first,denote the ground truth at round𝑡to be𝐺𝑡. Then, for the reward of agent 𝑖∈ [𝑁] at round 𝑡, we set

 1− 1+𝑛𝑡𝑡+𝑝𝑡, if𝑜𝑡1 ,𝑖= 𝐺𝑡= 0;

𝑝![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.023.png)

−1, if𝑜1 = 0,𝐺𝑡= 1;

𝑡,𝑖

 1, if𝑜𝑡1 ,𝑖= 𝐺𝑡= 1. (1) 𝑟𝑡,𝑖= −(1− 𝑝𝑡~~ ), if𝑜1 = 1,𝐺= 0;

 1+𝑛𝑡+𝑝𝑡 𝑡,𝑖 𝑡



The design of 𝑟𝑡,𝑖is to give a bigger reward or punishment for the prediction of the rarer positive samples. Otherwise, if 𝑟𝑡,𝑖∈ {−1,1}for all 𝑡and 𝑖, great chances are that the learning of rarer positive samples will be quickly offset by the major negative samples and the performance of MA-DGNN might hurt much accordingly.

2. Our customized cooperative MARL algorithm. We regard each user as an agent and design a graph-based MARL algorithm to make a comprehensive decision for each user. The minimax multi-agent deep deterministic policy gradient (M3DDPG) approach [[20\]](#_page18_x73.44_y304.26) is adopted to update our proposed algorithm, for the purpose of learning robust policies. Below we illustrate the structure of our method in detail.

Centralized critic and decentralized actors with minimax optimization. We take a centralized critic which absorbs the concatenation of all agents’ states and actions (decentralized actors). The critic, then, outputs the value vector which evaluates the state-action pair of each agent, so as to update the policy network.

Since during the training of MARL, agents’ policies might be very sensitive to the changes in other agents’ strategies, which could slow down the speed of policy loss’s convergence as well as make the decision-making process out-of-order. In order to learn more robust policies, we are the first ones in the field of mention recommendation who follow [[20\] ](#_page18_x73.44_y304.26)to update the policy and critic networks considering the worst situation, i.e. optimizing the reward of each agent while assuming other agents perform adversarially. Formally, we customize minimax objectives for each agent to update the

policy and critic networks at round 𝑡, where minimax is a fundamental concept in game theory and can be applied to general decision-making under uncertainty, prescribing a strategy that minimizes the possible losses for a worst-case scenario, as stated in [\[20](#_page18_x73.44_y304.26)]. Specifically, the minimax objectives of the policy network and the critic network for agent𝑖are

max min𝑄𝝁(s𝑡,𝑎0,...,𝑎𝑖,...,𝑎0 )|𝑎=𝝁(s ),

𝝁 𝑎0 𝑖 1 𝑁 𝑖 𝑖𝑡 (2)

𝑗≠𝑖

and

2

max − L 𝑡,𝑖= − 𝑄𝑖𝝁s𝑡,𝑎𝑡,1,...,𝑎𝑡,𝑁− 𝑦𝑖,

𝑦𝑖= 𝑟𝑡,𝑖+𝛾𝑄𝑖𝝁′ s𝑡+1,𝑎′★,...,𝑎𝑖′,...,𝑎𝑁′★ ,

1

𝑎𝑖′ = 𝝁𝑖′ (s𝑡+1), (3)

𝑗≠𝑖= argmin 𝑄𝑖𝝁′ s𝑡+1,𝑎1′,...,𝑎𝑁,

𝑎′★ ′

𝑎′𝑗≠𝑖

respectively. In [(2)](#_page7_x242.58_y139.99) and [(3)](#_page7_x238.41_y203.93), s𝑡is the concatenation of states for all agents at round 𝑡, 𝑎𝑡,𝑖(𝑖∈ [𝑁]) is the action of agent 𝑖at round 𝑡, 𝝁is the policy of all agents, 𝝁𝑖(st) is the 𝑖-th component of 𝝁(st), and 𝝁′ is the target policy network of

M3DDPG [\[20\].](#_page18_x73.44_y304.26)

To derive 𝑎0𝑗≠𝑖and 𝑎′★𝑗≠𝑖for each agent 𝑖∈ [𝑁], we adopt the end-to-end solution that approximates the 𝑄function by a locally linear function and replaces the inner-loop minimization in[(2)](#_page7_x242.58_y139.99) and [(3)](#_page7_x238.41_y203.93) with a 1-step gradient descent. Below

is the calculation procedure of 𝑎0 and 𝑎′★ :

𝑗≠𝑖 𝑗≠𝑖

𝑎𝑘= 𝝁𝑘(s𝑡), ∀1 ≤ 𝑘≤ 𝑁,

𝑎0𝑗= 𝑎𝑗+𝜖0𝑗, ∀𝑗≠ 𝑖,

𝜇 0

𝜖0𝑗≠𝑖= argmin 𝑄𝑖s𝑡,𝑎1 +𝜖10,...,𝑎𝑖,...,𝑎𝑁+𝜖𝑁

𝜖0

𝑗≠𝑖

𝝁

- −𝛼𝑗∇𝑎𝑗𝑄𝑖(s𝑡,𝑎1,...,𝑎𝑁) .

𝑎𝑘′ = 𝝁𝑘′ (s𝑡+1), ∀1 ≤ 𝑘≤ 𝑁, (4) 𝑎′★ = 𝑎′ +𝜖′, ∀𝑗≠ 𝑖,

𝑗 𝑗 𝑗

′ 𝜇′ ′

𝜖𝑗≠𝑖= argmin𝜖′ 𝑄𝑖s𝑡+1,𝑎1′ +𝜖1′,...,𝑎𝑖,...,𝑎𝑁′ +𝜖𝑁′

𝑗≠𝑖

𝝁′

- −𝛼𝑗∇𝑎′ 𝑄𝑖s𝑡+1,𝑎1′,...,𝑎𝑁′ .

𝑗

Afterobtainingthepolicyloss,i.e.,− min𝑎0 𝑄𝑖𝝁(s𝑡,𝑎01,...,𝑎𝑖,...,𝑎0𝑁|𝑎=𝝁(s )),andthecriticlossL𝑡,𝑖foreach𝑖∈ [𝑁],

𝑗≠𝑖 𝑖 𝑖𝑡

we linearly combine the policy losses for all agents and the critic losses for all agents, respectively, with the combination coefficientsdescribed in the below credit assignment part.

Delayed aggregation graph policy network. To model the dynamic and sparse communications between users and make a comprehensive decision, we adopt the delayed aggregation graph neural network (DGNN) [[36\] ](#_page18_x73.44_y563.29)as the policy network of MARL. This method was never used in recommendation systems and we novelly integrate it into our elaborately designed MA-DGNN which results in great experimental performances on various datasets. Thanks to different levels of information diffusion along the multi-hop neighbors in DGNN, the relationship between agents with sparse interactions might be well inferred from the flowing information among other agents with varying distances. Below, we introduce DGNN in detail.

Aggregation GNNs [[9\]](#_page17_x113.65_y630.36) are information processing frameworks which operate on network data in a decentralized manner, utilizing useful information through repeated exchanges with neighbors. Different from normal aggregation GNNs that deal with fixed graph signals defined over fixed graph support, Tolstaya et al. [[36\] ](#_page18_x73.44_y563.29)extend them to tackle time-varying graph processes set up over time-varying graph support.

Use (𝑖, 𝑗) ∈ E𝑛to represent that 𝑗might send data to 𝑖at time 𝑛. With these denotations, Tolstaya et al. [[36\]](#_page18_x73.44_y563.29) then describe the graph support𝐺by a graph shift operator𝑆𝑛∈R𝑁×𝑁where [𝑆] might not be zero only if (𝑗,𝑖) ∈ E or

𝑛 𝑛𝑖𝑗 𝑛

if 𝑖= 𝑗, indicating the sparsity of the graph. More specifically, denote 𝑥(𝑛−1) 𝑗= 𝑥⊤𝑗(𝑛−1) to be the state of node 𝑗at time 𝑛− 1, then through the property that [𝑆𝑛]𝑖𝑗might not be zero only if (𝑗,𝑖) ∈ E𝑛or if 𝑖= 𝑗, we obtain:

[𝑆𝑛𝑥𝑛−1]𝑖= [𝑆𝑛]𝑖𝑗[𝑥𝑛−1]𝑗,Nin={𝑗: (𝑖, 𝑗) ∈ E𝑛}. (5)

𝑗=𝑖,𝑗∈N𝑖𝑛

𝐾−1

N𝑖𝑘𝑛= 𝑗′ ∈N𝑗𝑘(−𝑛1−1) : 𝑗∈ N𝑖𝑛,H𝑖𝑛= 𝑥𝑗(𝑛−𝑘) : 𝑗∈N𝑖𝑛 (6)

𝑘.

𝑘=0

Based on the locality of[(5)](#_page8_x181.39_y206.89), delayed aggregation GNNs build a sequence of recursive𝑘-hop neighborhood aggregations

as shown in [(6),](#_page8_x179.30_y244.81) [(7),](#_page8_x249.63_y300.69) and [(8).](#_page8_x214.57_y361.47) Specificallyspeaking, definea sequence of signals𝑦𝑘𝑛∈R𝑁×𝑝with𝑦0𝑛= 𝑥𝑛and

𝑦𝑘𝑛= 𝑆𝑛𝑦(𝑘−1)(𝑛−1). (7) Then it holds that 𝑦𝑘𝑛= (𝑆𝑛𝑆𝑛−1...𝑆𝑛−𝑘+1)𝑥𝑛−𝑘. Therefore, [(7)](#_page8_x249.63_y300.69) is characterizing the diffusion of 𝑥𝑛−𝑘through the

series of time varying networks from𝑆𝑛−𝑘+1 to𝑆𝑛. Next, aggregate𝑦𝑘𝑛for𝑘∈ {0,1,···,𝐾− 1}to form the nested states along the multi-hop neighbors:

Z 𝑖𝑛= [𝑦0𝑛]𝑖;[𝑦1𝑛]𝑖;...; 𝑦(𝐾−1)𝑛𝑖, (8)

𝑘 where the𝑘+1-st element of𝑧𝑖𝑛, [𝑦𝑘𝑛]𝑖= [𝑆𝑛𝑆𝑛−1...𝑆𝑛−𝑘+1𝑥𝑛−𝑘]𝑖, is an average of𝑥𝑗(𝑛−𝑘) of𝑘-hop neighbors 𝑗∈ N𝑖𝑛

at time 𝑛− 𝑘.

Specifying that 𝑧𝑖𝑛has a regular temporal structure due to its nested aggregation property, Tolstaya et al. [[36\]](#_page18_x73.44_y563.29) model 𝑧𝑖𝑛by a convolutional neural network with length 𝐿, that is, for𝑙∈ [𝐿], set

Z (ℓ) = 𝜎(ℓ) H(ℓ)Z (ℓ−1) (9)

𝑖𝑛 𝑖𝑛

where 𝜎(ℓ) is a nonlinearity operator and H(ℓ) is a bank of small-support filtersshared across all nodes with learnable parameters.

To sum up, the delayed aggregation GNN architecture, characterized by [(5)](#_page8_x181.39_y206.89)-[(9)](#_page8_x239.89_y440.44), comprises a local parameteriza- tion of the policy 𝜋(H𝑖𝑛,H) which captures the dynamic and sparse interactions in a network and allows for distant communications through the multi-hop information diffusion.

Customized experience replay. To (i) absorb experience from users with larger similarity to other users and better decision quality for other users, and (ii) attach greater importance to the newer samples and the rarer positive samples, during the sampling process in MARL’s replay buffer, we are the first one to take all of the following factors into account: (i) the time when a sample occurs; (ii) whether the user feedback is positive or negative; (iii) the times that the target user in a sample mentions a person; (iv) the times that the target user is mentioned by other users; (v) the averaged performance that the target user assists on other users’ dec[isions6;](#_page8_x76.68_y635.67) and (vi) the averaged similarity between the target user’s state and other users’ states. We calculate the aforementioned six factors 𝑧0 ,𝑧0 ,···,𝑧0 for each sample

𝑖,1 𝑖,2 𝑖,6![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.024.png)

6Assume user 𝑖is the target user of a certain sample in replay buffer, and 𝑗is the time when the sample occurs. Then, the averaged performance that the target user assists on other users’ decisions is characterized by ( 𝑗 𝑟𝑘,𝑖)/ 𝑗.

𝑘=1

𝑖∈ [𝑁𝑏𝑢𝑓 𝑓𝑒𝑟]. With regard to each factor 𝑗∈ [6], we perform the softmax function on 𝑧10,𝑗,𝑧20,𝑗,···,𝑧𝑁0 ,𝑗and

𝑏𝑢𝑓 𝑓 𝑒𝑟 obtain 𝑧1,𝑗,𝑧2,𝑗,···,𝑧𝑁 ,𝑗. Then for each sampling process, for the 𝑖’s sample (𝑖∈ [𝑁𝑏𝑢𝑓 𝑓𝑒𝑟]) in the replay buffer, the corresponding sampling𝑏𝑢𝑓 𝑓 𝑒probability𝑟 is 6𝑗=1𝑧𝑖,𝑗. Note that to better approximate most of the actions and the state-action

6

pairs, it is necessary to update MA-DGNN with a rich variety of samples. However, since samples provided by the above method might demonstrate a strong tendency regarding the six indexes, the diversity of the training samples could be far from enough. Therefore, to boost the diversity of experience replay, we take the above sampling method for a part of

the training samples and the other part is obtained by state clustering. Specifically, we cluster the samples in the buffer into 10groups according to their state and select samples from each group uniformly at random with an equal amount. Furthermore, during the sampling process, we novelly remove a certain amount of samples according to the degree of samples’ non-stationarity. Specifically, we measure the standard error of the true labels for samples in each group and obtain the averaged value,𝑠𝑡𝑑𝑎𝑣𝑔,𝑡, of the 10standard errors. If𝑠𝑡𝑑𝑎𝑣𝑔,𝑡is larger than the historical{𝑠𝑡𝑑𝑎𝑣𝑔,𝑖}𝑖∈[𝑡−1], then we judge that there might exist obvious non-stationarity and thus remove the oldest min{100,𝑠𝑡𝑑𝑎𝑣𝑔,𝑡/100} percent of samples to keep track of the latest preferences of users.

Credit assignment. Recall that s𝑡is the concatenation of states for all agents at round𝑡, 𝑎𝑡,𝑖(𝑖∈ [𝑁]) is the action of agent𝑖, and 𝝁is the policy of all agents. For each agent𝑖∈ [𝑁], the policy loss is𝑄𝝁(s ,𝑎) ln𝜋(𝑎|s ), where 𝜋is the

𝑖𝑡𝑡 𝑡,𝑖𝑡

probability of obtaining𝑎𝑡,𝑖under state s𝑡and policy 𝝁. In the critic network and the policy network, we assign different weights to the critic loss and the policy loss of different agents and sum the losses to perform unifiedback-propagation. The weight coefficientsare the elements of the 𝑖-th row of the user-user utility matrix, just as stated below.

3. Design and update of user-user utility matrix. To make a comprehensive decision and perform the credit assignment in MARL, we construct a user-user utility matrix to obtain the weights of linear combinations. Since there are 𝑁2 parameters in the utility matrix which require much time to be correctly approximated, we assume the utility matrix can be factorized into two small matrices, 𝐴and 𝐵, with dimensions 𝑁× 𝑑and𝑑× 𝑁, respectively, where 𝑑is a small number relative to 𝑁. To estimate the two matrices, the neural matrix factorization method is utilized with its loss composed of the following two parts:
- algorithm.𝜇2 𝑛𝑝Σ𝑡𝑡++11 𝑡  Here,− 𝑎𝑡𝑝𝑡2and+ Σ𝑛𝑡serve to alle2viate the issue of the seriously imbalanced data.
- −[~~ 𝐺log(𝑜)) + (1−𝐺𝑡) log(1−𝑜𝑡))], where𝑜𝑡∈ [0,1] is the linearly weighted output of the graph-based MARL 𝑖𝑗𝑠𝑖𝑗𝑎𝑖 𝑗 𝑖𝑗𝑠𝑖𝑗𝑏𝑖− 𝑏𝑗2 , where 𝑠𝑖𝑗is the similarity between user 𝑖’s state and user 𝑗’s state, 𝑎𝑖is the![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.025.png)

2

𝑖-th row of the𝐴matrix, and𝑏𝑖is the𝑖-th column of the 𝐵matrix. We use this term to encourage that the more similar between user𝑖’s state and user 𝑗’s state, the more similar between user𝑖’s relationship to others and user𝑗’s relationship

to others.

Note that although there might exist a great deal of promising matrix factorization methods [[2,](#_page17_x113.65_y510.80) [2,](#_page17_x113.65_y510.80) [5,](#_page17_x113.65_y550.65) [5,](#_page17_x113.65_y550.65) [27\]](#_page18_x73.44_y443.73) and other algorithms appropriate to update the utility matrix, our MA-DGNN with the above neural matrix factorization mechanism has already achieved remarkable performances in extensive experiments. Here we leave the study of other approaches that optimize the utility matrix as the future work.

**4 DISCUSSION**

In this section, we illustrate several aspects that need to be noted for our proposed MA-DGNN algorithm.

- Q: Why do not we adopt the Gumbel-softmax sampling trick for DDPG to conduct discrete actions?

A: The difference between our method and the Gumbel-softmax sampling trick is that we compare the aggregated output with coefficientsderived by neural matrix factorization with a sliding-window threshold instead of perform- ing sampling according to each value output by each agent. For our MA-DGNN, if we adopt Gumbel-softmax sampling, then the neural matrix factorization technique could not be implemented due to the lack of the loss derived by the aggregated outputs and thus the credit assignment mechanism also could not be carried out, which significantlydegrades MA-DGNN’s performance in the experimental evaluation. Therefore, we do not consider the Gumbel-softmax sampling trick.

- Q: What are the motivation and advantages of each component in MA-DGNN?

A: (1) DGNN: The preferences of each user are dynamic, so are the interactions between users, which are very similar to the phenomenon of population evolution in a complex network. Based on this, we associate the DGNN (delayed aggregation graph neural network) algorithm, which is very suitable for modeling the population evolution of complex networks. DGNN is expected to capture the changes of each individual and of the relationships between individuals adaptively. Also, it is hoped that by using DGNN, the decisions of individuals with sparse interactions can be derived from individuals with richer interaction information.

2) MADDPG: When using a centralized evaluation function, the decision mechanism for each intelligence is influencednot only by the external dynamic environment but also by the changing decision mechanisms for other intelligence, which may make the decision updating process very unstable. In order to make the training process as stable and high-quality as possible, MADDPG assumes that all other intelligence make the worst decisions when optimizing each intelligence and expects the current intelligence to optimize its own decisions to the best under such a worst-case scenario. In summary, MADDPG is expected to help the algorithm keep optimizing in a good quality action space stably, rather than accidentally fall into a bad action space that is hard to get out of.
2) The customized replay buffer: Sample selection in MARL has a significantimpact on the effectiveness of the algorithm. The time of appearance of samples, the positivity or negativity samples, the number of historical appearances of corresponding users, the similarity between users, and the quality of each user’s assisted decision- making for other users are all factors that should be taken into account when performing sample selection. In this regard, we develop a prioritized experience replay mechanism based on six factors to select suitable samples for training. In addition, we also perform sample selection based on state clustering to promote sample diversity. Besides these, we also measure the non-stationarity of the recent environment based on the concentration of the sample pools in the same class and eliminate a certain amount of old samples according to the degree of non-stationarity. The carefully modulated sampling strategy results in real-time, high-quality policy updates, and we can see the clear advantages of this mechanism in ablation learning.
2) Neural matrix factorization: To tackle the credit assignment in MARL, we construct a user-user utility matrix and update it in real-time with a supervised learning algorithm. Due to the sparse information of some user-user interactions, it is difficultto accurately estimate the corresponding elements of the utility matrix using a normal deep neural network. In this regard, we adopt the neural matrix factorization that can cope well with the problem of missing data by learning hidden factors, and develop a loss function for mitigating the positive and negative sample imbalance while exploiting the similarity of user states to train neural matrix factorization, and we can see the great improvement of this technique from the latter-stated ablation study section.
2) Multi-hop information diffusion: The use of multi-hop information diffusion mechanisms helps users with sparse interactions to mention recommendations of candidates or entities obtained from users with dense interactions. Multi-hop information diffusion mechanism is a method that uses graph attention mechanisms and

diffusion techniques to enable multiple propagation and aggregation of information in the graph structure, thereby enhancing the representation and delivery of information.1 In the task of mention recommendation, multi-hop information diffusion mechanism can help users with sparse interactions to obtain suggestions from users with dense interactions, because users with dense interactions usually have richer information and stronger influence, which can provide more appropriate and valuable mention candidates or entities to users with sparse interactions. For example, suppose that on Twitter, User A and User B frequently mention each other, while User C and User D rarely mention each other, but User C is somewhat connected to User A, while User D is somewhat connected to User B. Then, when User C wants to mention a related user in a post, the multi-hop information diffusion mechanism can help User C get suggestions from User A’s mention history, thus improving the effect of mention recommendation. Similarly, when user D wants to mention a related user in a post, the multi-hop information diffusion mechanism can help user D to get suggestions from user B’s mention history, thus improving the effect of mention recommendation.

DAN DAN DAN![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.026.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.027.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.028.png)

AU-HMNN AU-HMNN AU-HMNN

CoA-CAMN CoA-CAMN 0.875 CoA-CAMN

NEM 0.80 NEM 0.850 NEM 0.800 G2ANet 0.75 G2ANet 0.825 G2ANet 0.775

MAAC 0.70 MAAC 0.800 MAAC 0.750 CROMA 0.65 CROMA 0.7500.775 CROMA 0.725

Ours 0.60 Ours 0.725 Recall Ours 0.700

Precision 0.700 0.675 F1-Score

0.55 0.675 0.650 C C C

B

A y 0 B y 0 B y 1 1 1

x 2 x A x

3 2 2 A

Fig. 3. Comparison results on the 9 synthetic datasets with K=3.

DAN DAN![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.029.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.030.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.031.png)

0.9 0.90

AU-HMNN 0.8 AU-HMNN

0.8 CoA-CAMN CoA-CAMN 0.85

NEM NEM

0.7

0.7 G2ANet G2ANet MAAC 0.80 MAAC

0.6

CROMA DAN 0.6 CROMA

0.5 Ours 0.75 AU-HMNN Ours CoA-CAMN

0.5

0.4 0.70 NEM

G2ANet

0.3 0.4

0.65 MAAC

CROMA

0.2

Ours

0.60 0.3

1 2 3 4 5 1 2 3 4 5 1 2 3 4 5

(a) Number of Recommended Users (b) Number of Recommended Users (c) Number of Recommended Users

0.8 DAN 0.90 DAN 0.8 DAN AU-HMNN![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.032.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.033.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.034.png) AU-HMNN AU-HMNN

0.7

CoA-CAMN 0.85 CoA-CAMN 0.7 CoA-CAMN NEM NEM NEM

0.6 G2ANet G2ANet G2ANet 0.80

MAAC MAAC 0.6 MAAC

0.5 CROMA CROMA CROMA Ours 0.75 Ours Ours

0.5

0.4

0.70

0.3 0.4

0.65

0.2

0.3

0.60

1 2 3 4 5 1 2 3 4 5 1 2 3 4 5

(d) Number of Recommended Users (e) Number of Recommended Users (f) Number of Recommended Users

Fig. 4. Precision, recall, and F1-Score in different number of recommended users. (a)(b)(c): performances on the Twitter dataset; (d)(e)(f): performances on the Sina Weibo dataset. Algorithms are run for 3 times.

**5 EXPERIMENTS**

In this section, we conduct extensive experiments on both the real-world and synthetic datasets to answer the following questions:

RQ1 HowdoesMA-DGNNperformonvariousdatasetsformentionrecommendation,comparedwithexistingbaselines? Table 2. Overall performance on the Twitter dataset. We repeat each experiment 3 times and report the average scores and standard errors (displayed in the brackets). The best results are given in bold. The higher the metric, the better performance of the model.

Twitter![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.035.png)

Model

Prec. Recall F1. MRR Hits@3 Hits@5 RM 0.6037 (0.0399) 0.5574 (0.0348) 0.5796 (0.0364) 0.5691 (0.0382) 0.5627 (0.0362) 0.5993 (0.0354) DAN 0.7237 (0.0441) 0.6231 (0.0182) 0.6696 (0.0319) 0.6402 (0.0284) 0.6287 (0.0281) 0.7103 (0.0298) AU-HMNN 0.7482 (0.0458) 0.6721 (0.0197) 0.7081 (0.0341) 0.6948 (0.0311) 0.6842 (0.0302) 0.7354 (0.0339 ) CoA-CAMN 0.7383 (0.402) 0.6449 (0.0193) 0.6893 (0.0327) 0.6695 (0.0259) 0.6541 (0.0251) 0.7346 (0.0291) NEM 0.7314 (0.0386) 0.6756 (0.0207) 0.7024 (0.0305) 0.6894 (0.0298) 0.6803 (0.0247) 0.7233 (0.0327) G2ANet 0.6533 (0.0442) 0.6539 (0.0248) 0.6284 (0.0327) 0.6795 (0.0334) 0.6665 (0.0292) 0.6406 (0.0356) MAAC 0.6781 (0.0476) 0.6376 (0.0297) 6689 (0.0385) 0.6548 (0.0304) 0.6461 (0.0312) 0.6735 (0.0374) CROMA 0.7080 (0.0437) 0.6621 (0.0228) 0.6974 (0.0336) 0.6832 (0.0314) 0.6725 (0.0261) 0.7027 (0.0355) MA-DGNN 0.8862 (0.0397) 0.7464 (0.0209) 0.8103 (0.0318) 0.7749 (0.0294) 0.7592 (0.0254) 0.8179 (0.0308)

Table 3. Overall performance on the Sina Weibo dataset.

Sina Weibo![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.036.png)

Model

Prec. Recall F1. MRR Hits@3 Hits@5 RM 0.6628 (0.0389) 0.6109 (0.0401) 0.6358 (0.0395) 0.6128 (0.0376) 0.6193 (0.0328) 0.6986 (0.0394) DAN 0.7749 (0.0498) 0.6502 (0.0312) 0.7071 (0.0428) 0.6443 (0.0391) 0.6551 (0.0301) 0.8341 (0.0418) AU-HMNN 0.8012 (0.0547) 0.7002 (0.0384) 0.7473 (0.0458) 0.6975 (0.0429) 0.7147 (0.0312) 0.9116 (0.0428) CoA-CAMN 0.7881 (0.0479) 0.6751 (0.0301) 0.7272 (0.0392) 0.6702 (0.0433) 0.6829 (0.0274) 0.8703 (0.0402) NEM 0.7803 (0.0519) 0.6875 (0.0349) 0.7310 (0.0472) 0.6987 (0.0445) 0.7263 (0.0329) 0.8291 (0.0462) G2ANet 0.7329 (0.0581) 0.6705 (0.0362) 0.7835 (0.0481) 0.6806 (0.0468) 0.7025 (0.0337) 0.83.49 (0.0479) MAAC 0.7532 (0.0558) 0.6634 (0.0364) 0.80.02 (0.0477) 0.6720 (0.0459) 0.6973 (0.0309) 0.8627 (0.0481) CROMA 0.7838 (0.0476) 0.6728 (0.0317) 0.8269 (0.0401) 0.6809 (0.0424) 0.7023 (0.0316) 0.8783 (0.0417) MA-DGNN 0.8879 (0.0513) 0.7853 (0.0313) 0.8335 (0.0418) 0.7851 (0.0421) 0.8195 (0.0308) 0.9341 (0.0441)

Table 4. Overall performance on the Twitter dataset for users with sparse interactions.

Twitter![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.037.png)

Model

Prec. Recall F1. MRR Hits@3 Hits@5 RM 0.4038 (0.0361) 0.3782 (0.0344) 0.3906 (0.0355) 0.3603 (0.0346) 0.3786 (0.0361) 0.5197 (0.0462) LambdaMart 0.5218 (0.0435) 0.5031 (0.0385) 0.5122 (0.0402) 0.4894 (0.0463) 0.4984 (0.0369) 0.6649 (0.0441) DAN 0.4828 (0.0387) 0.4595 (0.0394) 0.4708 (0.0389) 0.4398 (0.0384) 0.4452 (0.0406) 0.6702 (0.0405) AU-HMNN 0.5084 (0.0401) 0.4792 (0.0413) 0.4933 (0.0409) 0.4423 (0.0428) 0.4608 (0.0387) 0.6996 (0.0416) CoA-CAMN 0.4934 (0.0426) 0.4813 (0.0374) 0.4872 (0.0399) 0.4294 (0.0393) 0.4378 (0.0391) 0.6603 (0.0385) NEM 0.4915 (0.0387) 0.4736 (0.0415) 0.4824 (0.0401) 0.4401 (0.0418) 0.4484 (0.0375) 0.6302 (0.0406) G2ANet 0.4618 (0.0372) 0.4495 (0.0384) 0.4556 (0.0379) 0.4375 (0.0375) 0.4457 (0.0417) 0.6297 (0.0394) MAAC 0.4814 (0.0421) 0.4602 (0.0398) 0.4706 (0.0403) 0.4473 (0.0369) 0.4552 (0.0433) 0.6583 (0.0371) CROMA 0.5097 (0.0386) 0.4831 (0.0416) 0.4960 (0.0396) 0.4571 (0.0414) 0.4638 (0.0396) 0.6589 (0.0428) MA-DGNN 0.6618 (0.0378) 0.6487 (0.0409) 0.6552 (0.0392) 0.5725 (0.0391) 0.5784 (0.0387) 0.7495 (0.0397)

RQ2 How do different configurationsof hyper-parameters and different indexes in the prioritized replay buffer influence

the overall performance of our proposed algorithm?

RQ3 What are the influencesof each component in MA-DGNN, such as the credit assignment procedure, the mimimax

optimization of the critic losses, etc.?

Table 5. Overall performance on the Sina Weibo dataset for users with sparse interactions.

Sina Weibo![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.038.png)

Model

Prec. Recall F1. MRR Hits@3 Hits@5 RM 0.4323 (0.0406) 0.3947 (0.0397) 0.4126 (0.0402) 0.3714 (0.0379) 0.3891 (0.0364) 0.5371 (0.0427) LambdaMart 0.5572 (0.0375) 0.5381 (0.0391) 0.5475 (0.0389) 0.4891 (0.0385) 0.5084 (0.0397) 0.7281 (0.0445) DAN 0.5193 (0.0435) 0.4794 (0.0398) 0.4986 (0.0417) 0.4615 (0.0391) 0.4672 (0.0381) 0.6983 (0.0375) AU-HMNN 0.5382 (0.0394) 0.4823 (0.0371) 0.5087 (0.0384) 0.4732 (0.0402) 0.4823 (0.0418) 0.7292 (0.0381) CoA-CAMN 0.5294 (0.0403) 0.4685 (0.0348) 0.4971 (0.0381) 0.4489 (0.0375) 0.4509 (0.0394) 0.6827 (0.0416) NEM 0.5271 (0.0391) 0.4703 (0.0361) 0.4971 (0.0386) 0.4632 (0.0329) 0.4639 (0.0402) 0.6494 (0.0427) G2ANet 0.4895 (0.0324) 0.4617 (0.0413) 0.4752 (0.0409) 0.4583 (0.0423) 0.4595 (0.0379) 0.6582 (0.0395) MAAC 0.4931 (0.0457) 0.4763 (0.0381) 0.4846 (0.0421) 0.4672 (0.0374) 0.5387 (0.0342) 0.6802 (0.0418) CROMA 0.5374 (0.0413) 0.4894 (0.0362) 0.5123 (0.0392) 0.4826 (0.0387) 0.5491 (0.0371) 0.6855 (0.0373) MA-DGNN 0.6707 (0.0394) 0.6182 (0.0328) 0.6434 (0.0371) 0.6094 (0.0342) 0.6561 (0.0421) 0.7507 (0.0379)

Table 6. Ablation study on the Twitter dataset.



|Metric|Dataset|MA-DGNN|𝑡ℎ𝑟𝑒𝑡= 0.5|w/o CER|w/o SBL|w/o CRA|w/o MMO|
| - | - | - | - | - | - | - | - |
|F1-Score|Twitter Sina|0.8103 0.8335|0.6708 0.7129|0.7506 0.7439|0.7834 0.7902|0.7681 0.7792|0.7792 0.7875|
|Hit@3|Twitter Sina|0.6803 0.8195|0.6175 0.6982|0.6189 0.7473|0.6695 0.7928|0.6417 0.7686|0.6579 0.7802|
|Hit@5|Twitter Sina|0.8179 0.9341|0.6951 0.8017|0.7428 0.8517|0.7826 0.9021|0.7606 0.8693|0.7693 0.8838|
0.82 0.90 ![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.039.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.040.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.041.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.042.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.043.png)

0.825 I\_1 Precision

0.81 I\_2 0.80 0.88 0.84 Recall

0.800 I\_3 F1-Score

0.80 I\_4 0.78 0.86

0.775 I\_5 0.82

0.79 I\_6 0.84

0.750 0.76

0.78 0.82 0.80

0.77 I\_1 0.725 0.74 I\_1

I\_2 I\_2 0.80

0.76 I\_3 0.700 I\_3 0.78

I\_4 0.72 I\_4 0.78 Precision

0.75 I\_5 0.675 I\_5 Recall 0.76

I\_6 0.70 I\_6 0.76 F1-Score

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.2 0.4 1.0 2.0 4.0 10.0 20.0 10 20 30 40 60 80 100

(a) n (b) n (c) n (d)  : coefficient of the similarity-based loss (e) number of clustering

Fig. 5. Hyper-parameter analysis on the Twitter dataset. (a)(b)(c): analysis of six indexes in replay buffer; (d) analysis of 𝜇, the coefficient of the similarity-based loss in neural matrix factorization; (e) analysis of the number of sample clusters in replay buffer.

1. **Experiment Setup**
1. Dataset. We evaluate our proposed MA-DGNN and the selected baseline models on both the real-world and synthetic datasets.

Real world datasets. We adopt two real datasets, the Twitter public dataset [\[10\]](#_page18_x73.44_y95.04) and the crawled Sina Weibo [dataset7, ](#_page13_x113.26_y644.85)from the two most popular social network platforms, Twitter and Sina Weibo. The statistics of the real-world datasets are shown in Table[ 7.](#_page14_x73.44_y89.56)

Synthetic datasets. As for the construction of synthetic datasets, for 𝑖∈ [𝑁] and 𝑡∈ [𝑇], we set the feature

(𝑐𝑜𝑠(𝑢𝑖,1𝑡),···,𝑐𝑜𝑠(𝑢𝑖,𝑑𝑡))

vector of user 𝑖at time 𝑡to be: 𝜃𝑖,𝑡= ||(𝑐𝑜𝑠(𝑢𝑖,1𝑡),···,𝑐𝑜𝑠(𝑢𝑡))||~~ , where 𝑑is the dimension of the feature vectors and

𝑖,𝑑 2

{𝑢𝑖,𝑗|𝑖∈ [𝑁], 𝑗∈ [𝑑]}are the constant coefficientsin different components.

For each𝑡∈ [𝑇], we randomly generate a tweet𝑡𝑤𝑡for the currently served user𝑖𝑡. The feature vector of𝑡𝑤𝑡is set to be 𝑣𝑡, where𝑣𝑡,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0,1]) for∀𝑗∈ [𝑑]. At this time, the user mentioned by user𝑖𝑡isargmax𝑗∈[𝑁] 𝜃⊤ 𝑡𝑤−![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.044.png)

||𝑣||𝑡 𝑗,𝑡𝑡

7We crawl the Sina Weibo according to [\[4\]](#_page17_x113.65_y540.69) and process the crawled dataset through the same procedures as the literature [\[24\].](#_page18_x73.44_y383.96)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.045.png)

Table 7. The dataset statistics.



||Twitter|Sina Weibo|Synthetic|
| :- | - | - | - |
|#Twitters / #Weibo|200,465|56,673|60,000|
|#Images|200,465|60,652|0|
|#Users|15,539|2,000|2,000|
|#Avg. Mention per User|39.45|36.23|41.77|
|#Avg. Mentioned per User|7.51|6.97|10.32|
||𝜃𝑗,𝑡− 𝜃𝑖,𝑡||2, i.e., the more similar to user𝑖𝑡a user is and the larger projection length on the vector𝑡𝑤𝑡a user owns, the

𝑡

more likely he/she is to be mentioned by user 𝑖𝑡at time 𝑡.

To demonstrate the robustness of our proposed MA-DGNN, we construct synthetic datasets with different degrees of non-stationarity and different distributions of users’ appearing frequency. Specifically, with regard to users’ varying preferences, i.e., the non-stationarity, we classify the synthetic datasets into 3 categories:

(𝑎) :𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 𝐼: 𝑓𝑜𝑟 𝑖∈ [𝑁], 𝑗∈ [𝑑],𝑢𝑖,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0, 310𝑇]); 

(𝑐) :𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 𝐼𝐼𝐼: 𝑓𝑜𝑟 𝑖∈ [𝑁], 𝑗∈ [𝑑],𝑢𝑖,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0, 𝑇]).

20

(𝑏) :𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 𝐼𝐼: 𝑓𝑜𝑟 𝑖∈ [𝑁], 𝑗∈ [𝑑],𝑢𝑖,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0, 3𝑇]);

10



Regarding users’ appearing frequency, we also classify the synthetic datasets into3 categories: (a) category A: at each time 𝑡∈ [𝑇], each user is uniformly randomly served, i.e., each user issues a tweet with the same probability at each round; (b) category B: users are served in a totally randomized way. (c) category C: we randomly select 10percent of users to issue approximately60percent of tweets. And in the rest rounds, the rest users are served in a totally randomized way.

We cross-link categories I, II, III with categories A, B, C and thus construct 9 (= 3× 3) synthetic datasets.

2. Baselines. The existing mention recommendation models can be categorized into two groups: the supervised learning-based (SL-based) group and the multi-agent reinforcement learning-based (MARL-based) group. As shown in Table[ ](#_page12_x73.44_y89.56)[2,3,](#_page12_x73.44_y264.05) we consider DAN [[25\],](#_page18_x73.44_y403.88) AU-HMNN [[15\],](#_page18_x73.44_y204.63) CoA-CAMN [[24\]](#_page18_x73.44_y383.96) and NEM [[42\]](#_page19_x110.16_y134.89) as representative SL-based baselines, and incorporate each of them with the actor-critic method[^3][ ](#_page14_x76.68_y643.77)for fair comparison with our reinforcement learning approach. For MARL-based methods, we choose G2ANet [\[23](#_page18_x73.44_y364.03)], MAAC [\[17\]](#_page18_x73.44_y244.48) and CROMA [\[11\]](#_page18_x73.44_y114.97) as baselines. Below, we describe our selected baselines in brief:
1) DAN. Dual-attention (DAN) is an attention method introduced in [[25\]](#_page18_x73.44_y403.88) which models the relationship between

objects from different perspectives.

2) AU-HMNN. AU-HMNN is proposed in [\[15](#_page18_x73.44_y204.63)], which incorporates the textual information of query tweets and user

histories. This is a state-of-the-art approach to the mention recommendation task.

3) CoA-CAMN. To use textual and visual information, Renfeng et al. [\[24\]](#_page18_x73.44_y383.96) propose a novel cross-attention memory

network to perform the mention recommendation task for multimodal tweets.

4) NEM. Yi et al. [[42\]](#_page19_x110.16_y134.89) construct a heterogeneous mention network according to different relationships among

different entities, and then infer a unified low dimensional embedding for users and messages by taking the network structure and vertex content information into account.![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.046.png)

5) G2ANet. Yong et al. [[23\]](#_page18_x73.44_y364.03) model the relationship between agents by a complete graph and propose a novel

game abstraction mechanism based on the integration of a two-stage attention network (G2ANet) with the multi-agent reinforcement learning training mechanism.

6) MAAC. MAAC [\[17\]](#_page18_x73.44_y244.48) is an actor-critic algorithm that trains decentralized policies in multi-agent settings, using

centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments when compared to recent approaches.

7) CROMA. CROMA [[11\]](#_page18_x73.44_y114.97) is a novel cooperative multi-agent approach to mention recommendation, which

incorporates dozens of more historical tweets than earlier approaches.

(viii) LambdaMart. LambdaMart [[3\]](#_page17_x113.65_y530.73) is a Listwise-learning-to-rank algorithm that transforms the ranking problem into a regression decision tree problem. As a classic learning-to-rank method, it is still widely applied in the search engine and recommendation systems of major Internet companies.

(ix) RM. The most recent K users that user 𝑖𝑡(i.e. the user currently being served) mentioned.

3. Evaluation Metrics. Following previous works [[11,](#_page18_x73.44_y114.97)[ 23,](#_page18_x73.44_y364.03)[ 42](#_page19_x110.16_y134.89)], we take the Precision, Recall, F1-score [[32](#_page18_x73.44_y523.44)], and Mean Reciprocal Rank (MRR) [\[28\]](#_page18_x73.44_y463.66) as evaluation metrics for the highest-ranked result. Furthermore, Hits@3 and Hits@5

   15. are reported to denote the percentage of correct results recommended from the top-K results. Higher values of these metrics indicate better performance of the mention recommendation model.
3. Implementation Details. In this work, We use the first80% of the dataset in time order as the training set and the

last 20% as the test set. We perform hyperparametric search on the training set for all baseline methods (with supervised

algorithms using 5-fold cross-validation). For supervised algorithms we train on a rolling basis every 5% of the full

dataset length on the test set; for reinforcement learning algorithms we train from the very beginning of the dataset. In the

end we measure the average performance of all algorithms in the test set.

As for the parameter setting of MA-DGNN, we set the discounted factor𝛾of MDP to be0.99, the length of𝑆𝑒𝑞𝐶𝑜𝑛𝑣’s input 𝑁𝑙𝑖𝑠𝑡to be 10, the perturbations 𝛼𝑖(𝑖∈ [𝑁]) during minimax optimization to be 10−5, the number of hops in the

decentralized actors to be2, and the number of sample clusters in the customized replay buffer to be20. The coefficientof

the similarity-based loss (i.e.,𝜇) in the neural matrix factorization component is set to be0.4. Moreover, the learning rates

for the actor network, the critic network, and the neural matrix factorization mechanism are set to be 10−5, 10−4, and

10−4, respectively. Our model implemented on PyTorch is available for revie[wers ](#_page15_x113.40_y644.31)[^4]and will be publicly available upon the

acceptance of this work.

2. **Overall Performance (RQ1)**

We first perform each mention recommendation model on two real-world datasets, i.e., Twitter and Sina Weibo. The results are presented in Table[ ](#_page12_x73.44_y89.56)[2,3,](#_page12_x73.44_y264.05) and Figure[ 4,](#_page11_x110.16_y372.62) from which we can obtain the following observations:

1) MA-DGNN significantlyoutperforms all the baseline models, especially for the Precision metric. Such improvement

validates the effectiveness of applying MARL with the delayed aggregation graph neural network, which ensures the personalized recommendation for each target user and the in-time adjustment for users’ dynamic preferences.

2) As for the performance analysis of other baselines, SL-based methods generally outweigh MARL-based methods

exceptMA-DGNN,whichformsavividcontrasttothesuperiorperformanceofMA-DGNNandconsistentlydemonstrates

theeffectivenessofourproposedcomponents(DGNN,thecustomizedpriorityreplaybuffer,andtheminimaxoptimization technique) in MA-DGNN to improve over traditional MARL methods. Therefore, MA-DGNN enables a better MARL- policy learning process and gains significantperformance improvement.

To further evaluate MA-DGNN and baselines under different degrees of non-stationarity and user-appearing frequency, we construct 9 synthetic datasets and report the Precision, Recall and F1-Score for each model. The results are shown in Figure[ 3,](#_page11_x110.16_y263.89) where our MA-DGNN shows absolute competence against other baselines in all9 synthetic datasets, especially when the user appearance is non-uniform and the environment is highly non-stationary.

In Figure[ 4,](#_page11_x110.16_y372.62) we show the Precision, Recall, and F1-score of different algorithms with the number of recommended users ranging from one to five. From Figure[ 4,](#_page11_x110.16_y372.62) the performances of MA-DGNN rank the highest regarding all the three metrics in all real datasets. Especially for the recall metric, however we change the number of users between one and five, the average recall rate of our MA-DGNN is consistently above 0.75, which shows absolute competence compared with other baseline methods.

3. **Sparse Users Analysis**

To test the advantage of dealing with sparse users’ interactions for MA-DGNN, on the Twitter and Sina Weibo datasets, we only select users whose Ag. Mention times is less than 20 percent of the mean times or users whose Ag. Mentioned times is less than 20 percent of the mean times. We train MA-DGNN and all baselines on the selected users and the performances are shown in Table[ 4 ](#_page12_x73.44_y424.28)and[ 5.](#_page13_x110.16_y89.56) As can be seen in Table[ 4](#_page12_x73.44_y424.28) and[ 5,](#_page13_x110.16_y89.56) our MA-DGNN still beats all other baselines whether on the Twitter dataset or the Sina Weibo dataset, which shows the strong capability of MA-DGNN to tackle with users with less interaction information.

4. **Hyper-parameter analysis (RQ2)**

In this section, we test the influenceof different configurationsof hyper-parameters on the overall performance in the Twitter dataset. Recall that 𝜇is the coefficientof the similarity-based loss in our neural matrix factorization module and we define𝑐𝑛to be the number of sample clustering in the replay buffer.

1) Set𝑐𝑛= 20and 𝜇= 0.2,0.4,1,2,4,10,20. Results under different values of 𝜇are shown in Figure[ 5 ](#_page13_x110.16_y365.27)(d).
1) Set 𝜇= 0.4 and𝑐𝑛= 10,20,30,40,60,80,100. Results under different values of𝑐𝑛are shown in Figure[ 5 ](#_page13_x110.16_y365.27)(e)[^5][. ](#_page16_x79.01_y636.01)From Figure[ 5 ](#_page13_x110.16_y365.27)(d)(e), we can see that the overall performances of MA-DGNN are positively correlated to the coefficient

of the similarity-based loss and applying an appropriate number of sample clusters in the replay buffer also improves MA-DGNN significantly

Next, we study the influenceof different indexes in the prioritized replay buffer on the overall performance in detail. For each 𝑚∈ [6] and each 𝑛∈ [6], we first fix 𝑚and 𝑛, and then set the sampling probability for the 𝑖’s sample to

be 6𝑗=1𝑧𝑖,𝑗+(𝑛−1)𝑧𝑖,𝑚. By varying 𝑚∈ [6] and 𝑛∈ [6], we can obtain the performances of our proposed model under 6

different proportions of the six indexes in the replay buffer (See Figure [5 ](#_page13_x110.16_y365.27)(a)(b)(c) for the Twitter dataset and Figure [5 ](#_page13_x110.16_y365.27)(a)(b)(c) for the Sina Weibo dataset with the number of recommended user set to be 1). From Figure[ 5,](#_page13_x110.16_y365.27) we can see that increasing the proportions of the indexes (ii)(v)(vi) could contribute to improving the performance regarding the recall and F1-score metrics. The performance with regard to Precision also does not hurt much when we pay more attention to indexes (ii)(v)(vi). These demonstrate the importance of whether the user feedback is positive or negative, the averaged

performance that a user assists on other users’ decisions, and the averaged similarity between a user’s state and other users’ states in boosting the prioritized experience replay mechanism.

5. **Ablation study (RQ3)**

For the ablation study, we test our method when 𝑡ℎ𝑟𝑒𝑡= 0.5 and in the lack of the customized experience replay (CER) mechanism, the similarity-based loss in neural matrix factorization (SBL), the credit assignment procedure (CRA), and the mimimax optimization of the critic losses (MMO), separately, on both the Twitter dataset and the Sina Weibo

dataset. As shown in Table[ 6,](#_page13_x110.16_y250.07) no matter for F1-score, hits@3, or hits@5, replacing𝑡ℎ𝑟𝑒𝑡= 𝑛𝑡+1 with𝑡ℎ𝑟𝑒𝑡= 0.5 and the

𝑝𝑡+1

lack of any component bring varying degrees of reduction in algorithm performance, which consistently demonstrates the Precision of algorithm design and the importance of tacit cooperation between different components. The obvious

𝑛+1

performance deterioration when replacing 𝑡ℎ𝑟𝑒𝑡= 𝑝𝑡𝑡+1 with 𝑡ℎ𝑟𝑒𝑡= 0.5 and when lacking the customized experience replay component specificallyshow the effectiveness and efficiency of our design of 𝑡ℎ𝑟𝑒𝑡, and our selected six indexes and the non-stationary detection mechanism with state clustering in the replay buffer.

**6 CONLUSION**

Mention recommendation is an important tool in social networking platforms to boost users’ experience. To deal with the dynamic and sparse interactions between users which are seldom considered by existing literature, we apply multi-agent reinforcement learning with customized prioritized experience replay to model users’ varying relationship and employ a delayed aggregation graph neural network as the decentralized actor. A user-user utility matrix optimized by neural matrix factorization with a similarity-based loss is designed to help make a comprehensive decision and perform credit assignment. In the experimental evaluation, apart from two real-world datasets, we also construct various synthetic datasets with different degrees of non-stationarity and distributions of users’ appearing frequency. Extensive experiments demonstrate the effectiveness of our design in characterizing users’ relationship and our method which has expressive advantages over existing methods. In the future, we plan to improve the explainability of MA-DGNN and its application in more specific scenarios, such as scenarios where different users’ social circles are extremely imbalanced. [Yan: if possible, some future directions could be put here.][Hanchi: yes madam!]

**REFERENCES**

1. M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning based recommender systems: A survey. Comput. Surveys55, 7 (2022), 1–38.
1. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. 2019. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems32 (2019), 7413–7424.
1. Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.
1. Lei Chen. [n.d.]. A tool for crawling Sina Weibo.[ https://github.com/dataabc/weiboSpider.](https://github.com/dataabc/weiboSpider)
1. Yuejie Chi, Yue M Lu, and Yuxin Chen. 2019. Nonconvex optimization meets low-rank matrix factorization: An overview. IEEE Transactions on Signal Processing 67, 20 (2019), 5239–5269.
1. Kevin Dabérius, Elvin Granat, and Patrik Karlsson. 2019. Deep Execution-Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks. Available at SSRN 3374766(2019).
1. Zhaoyun Ding, Xueqing Zou, Yueyang Li, Su He, Jiajun Cheng, Fengcai Qiao, and Hui Wang. 2016. Mentioning the optimal users in the appropriate time on Twitter. In Asia-PacificWeb Conference. Springer, 464–468.
1. Laura Fontanesi, Sebastian Gluth, Mikhail S Spektor, and Jörg Rieskamp. 2019. A reinforcement learning diffusion decision model for value-based decisions. Psychonomic bulletin & review26, 4 (2019), 1099–1121.
1. Fernando Gama, Antonio G Marques, Alejandro Ribeiro, and Geert Leus. 2019. Aggregation graph neural networks. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 4943–4947.
10. Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua Zhou, and Xuanjing Huang. [n.d.]. Pytorch implementation of Mention Recommen- dation in Twitter with Cooperative Multi-Agent Reinforcement Learning.[ htps://github.com/mritma/CROMA.](htps://github.com/mritma/CROMA)
10. Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua Zhou, and Xuanjing Huang. 2019. Mention recommendation in Twitter with cooperative multi-agent reinforcement learning. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 535–544.
10. Taehyun Ha, Seunghee Han, Sangwon Lee, and Jang Hyun Kim. 2017. Reciprocal nature of social capital in Facebook: an analysis of tagging activity. Online Inf. Rev.41, 6 (2017), 826–839.
10. Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, and Zhan Chen. 2018. Playing 20 question game with policy-based reinforcement learning. arXiv preprint arXiv:1808.07645 (2018).
10. Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu. 2018. Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 368–377.
10. Haoran Huang, Qi Zhang, Xuanjing Huang, Haoran Huang, Qi Zhang, and Xuanjing Huang. 2017. Mention Recommendation for Twitter with End-to-end Memory Network.. In IJCAI. 1872–1878.
10. Jeff Huang, Katherine Thornton, and Efthimis N. Efthimiadis. 2010. Conversational tagging in twitter. In HT’10, Proceedings of the 21st ACM Conference on Hypertext and Hypermedia, Toronto, Ontario, Canada, June 13-16, 2010, Mark H. Chignell and Elaine G. Toms (Eds.). ACM, 173–178.
10. Shariq Iqbal and Fei Sha. 2019. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine Learning. PMLR, 2961–2970.
10. Bo Jiang, Ying Sha, and Lihong Wang. 2015. Predicting user mention behavior in social networks. In Natural Language Processing and Chinese Computing. Springer, 146–158.
10. Quanle Li, Dandan Song, Lejian Liao, and Li Liu. 2015. Personalized mention probabilistic ranking–recommendation on mention behavior of heterogeneous social network. In International conference on web-age information management. Springer, 41–52.
10. Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. 2019. Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 33. 4213–4220.
10. Yuanguo Lin, Yong Liu, Fan Lin, Lixin Zou, Pengcheng Wu, Wenhua Zeng, Huanhuan Chen, and Chunyan Miao. 2021. A survey on reinforcement learning for recommender systems. arXiv preprint arXiv:2109.10665 (2021).
10. Feng Liu, Ruiming Tang, Xutao Li, Yunming Ye, Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2018. Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling. CoRRabs/1810.12027 (2018). arXi[v:1810.12027](https://arxiv.org/abs/1810.12027)[ http://arxiv.org/abs/1810.12027](http://arxiv.org/abs/1810.12027)
10. Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. 2020. Multi-agent game abstraction via graph attention neural network. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 34. 7211–7218.
10. RenfengMa,QiZhang,JiawenWang,LizhenCui,andXuanjingHuang.2018. Mentionrecommendationformultimodalmicroblogwithcross-attention memory network. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 195–204.
10. Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. 2017. Dual attention networks for multimodal reasoning and matching. In Proceedings of the IEEE conference on computer vision and pattern recognition. 299–307.
10. Soumajit Pramanik, Mohit Sharma, Maximilien Danisch, Qinna Wang, Jean-Loup Guillaume, and Bivas Mitra. 2019. Easy-Mention: a model-driven mention recommendation heuristic to boost your tweet popularity. International Journal of Data Science and Analytics 7, 2 (2019), 131–147.
10. Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the eleventh ACM international conference on web search and data mining. 459–467.
10. Dragomir R Radev, Hong Qi, Harris Wu, and Weiguo Fan. 2002. Evaluating Web-based Question Answering Systems.. In LREC. Citeseer.
10. Muhammad Rehan Raza, Carlos Natalino, Peter Öhlen, Lena Wosinska, and Paolo Monti. 2018. A slice admission policy based on reinforcement learning for a 5G flexible RAN. In 2018 European Conference on Optical Communication (ECOC). IEEE, 1–3.
10. Saiph Savage, Andrés Monroy-Hernández, Kasturi Bhattacharjee, and Tobias Höllerer. 2015. Tag Me Maybe: Perceptions of Public Targeted Sharing on Facebook. CoRRabs/1509.01095 (2015). arXi[v:1509.01095](https://arxiv.org/abs/1509.01095)
10. Mohit Sewak. 2019. Policy-Based Reinforcement Learning Approaches. In Deep Reinforcement Learning. Springer, 127–140.
10. Guy Shani and Asela Gunawardana. 2011. Evaluating recommendation systems. In Recommender systems handbook. Springer, 257–297.
10. Shobrook. [n.d.]. SeqConv, a PyTorch implementation of a LSTM-based graph convolutional operator.[ https://pypi.org/project/seq-conv/.](https://pypi.org/project/seq-conv/)
10. Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.
10. Liyang Tang, Zhiwei Ni, Hui Xiong, and Hengshu Zhu. 2015. Locating targets through mention in Twitter.World Wide Web18, 4 (2015), 1019–1049.
10. Ekaterina Tolstaya, Fernando Gama, James Paulos, George Pappas, Vijay Kumar, and Alejandro Ribeiro. 2020. Learning decentralized controllers for robot swarms with graph neural networks. In Conference on Robot Learning. PMLR, 671–682.
10. Beidou Wang, Can Wang, Jiajun Bu, Chun Chen, Wei Vivian Zhang, Deng Cai, and Xiaofei He. 2013. Whom to mention: expand the diffusion of tweets by@ recommendation on micro-blogging systems. In Proceedings of the 22nd international conference on World Wide Web. 1331–1340.
10. Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 1225–1234.
10. Kai Wang, Weiyi Meng, Shijun Li, and Sha Yang. 2019. Multi-Modal Mention Topic Model for mentionee recommendation. Neurocomputing 325 (2019), 190–199.
40. Xuesong Wang, Yang Gu, Yuhu Cheng, Aiping Liu, and CL Philip Chen. 2019. Approximate policy-based accelerated deep reinforcement learning. IEEE transactions on neural networks and learning systems 31, 6 (2019), 1820–1830.
40. Tengyu Xu and Yingbin Liang. 2021. Sample complexity bounds for two timescale value-based reinforcement learning algorithms. In International Conference on ArtificialIntelligence and Statistics. PMLR, 811–819.
40. Feng Yi, Bo Jiang, and Jianjun Wu. 2020. Heterogeneous information network embedding for mention recommendation. IEEE Access8 (2020), 91394–91404.
40. Mengran Yu and Shiliang Sun. 2020. Policy-based reinforcement learning for time series anomaly detection. Engineering Applications of Artificial Intelligence 95 (2020), 103919.
40. Xinshi Zang, Huaxiu Yao, Guanjie Zheng, Nan Xu, Kai Xu, and Zhenhui Li. 2020. Metalight: Value-based meta-reinforcement learning for traffic signal control. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 34. 1153–1160.
40. Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems. 95–103.
40. Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with negative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1040–1048.
40. Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework for news recommendation. In Proceedings of the 2018 World Wide Web Conference. 167–176.
40. Ge Zhou, Lu Yu, Chu-Xu Zhang, Chuang Liu, Zi-Ke Zhang, and Jianlin Zhang. 2015. A novel approach for generating personalized mention list on micro-blogging system. In 2015 IEEE international conference on data mining workshop (ICDMW). IEEE, 1368–1374.![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.047.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.048.png)![](Aspose.Words.79bfefec-3bfa-45f1-b8a0-16a83b8d5611.049.png)
21

[^1]: Although Gui et al. [\[11\]](#_page18_x73.44_y114.97) propose a MARL method to tackle mention recommendation problems, the method only constructs two agents, i.e., two tweet selectors with different functions instead of modeling each user as an individual agent, and thus neglects users’ personal preference.
[^2]: In experiments we recommend the candidate mentionees from the whole user set[𝑁] for each target user because the times that MA-DGNN recommends user𝑖𝑡when user𝑖𝑡is issuing a message is too rare to influencethe ultimate performances.
[^3]: We take the supervised method as the policy network and the critic network of actor-critic.
[^4]: Code: https://filedropper.com/d/s/dLEu015PoqARaoZOWSIgu9J3LKYbXL .
[^5]: Although MA-DGNN is somewhat sensitive to𝑐𝑛, it has consistently great performances which outweigh the second best method in a wide range of𝑐𝑛in the Twitter dataset. Therefore, we do not need to overly worry about the setting of𝑐𝑛.
