ACM Transactions on Recommender Systems

**Multi-agent Reinforcement Learning for Mention Recommendation**



|Journal:|*ACM Transactions on Recommender Systems*|
| - | - |
|Manuscript ID|TORS-2022-0043|
|Manuscript Type:|Original Research Paper|
|<p>Date Submitted by the </p><p>Author:</p>|02-Nov-2022|
|Complete List of Authors:|<p>HUANG, HANCHI; Shanghai Jiao Tong University, Li, Yan; Shenzhen Polytechnic</p><p>Shen, Li; JD.com Inc</p><p>Lin, Jianghao; Shanghai Jiao Tong University</p><p>Xin, Xin; Shandong University</p>|
|<p>Computing Classification </p><p>Systems:</p>|Mention Recommendation, Multi-agent Reinforcement Learning, Graph Neural Network|
||
![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.001.png)

Page 1 of 20 ACM Transactions on Recommender Systems

1

2

3 Dear Editor:

4

5

6 Weareveryinterestedin submittinga researchpaperto [ACMTransactionson Recommender](https://dl.acm.org/journal/tors)

7 Systems.

8

9

10 The titleof the researchpaperis:“ Multi-agentReinforcementLearningfor Mention

11 Recommendation” . No conflictof interestexistsin the submissionof this manuscript,

12 and the manuscriptisapprovedbyallauthors for publication.I wouldliketo declareon behalfof 13

14 myco-authorsthat the workdescribedwasoriginalresearchthat hasnot been publishedpreviously, 15 and not under considerationfor publicationelsewhere,in wholeor inpart. Allthe authorslisted

16 haveapprovedthe manuscriptthat isenclosed.

17

18

19 Thankyouverymuch for consideringour manuscriptfor potentialpublication.Pleasedo not

20 hesitateto contact me withanyproblems.

21

22

23 Bestregards,

24 HANCHI HUANG, ShanghaiJiaoTong University,

25 YANLI,ShenzhenPolytechnic,

26

27 LI SHEN, JD Explore Academy,

28 JIANGHAO LIN, ShanghaiJiaoTong University,

29 XIN XIN, ShandongUniversity.

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59 60

ACM Transactions on Recommender Systems 61 of 20

1 **111** 2

3

4

5

6 **Multi-agent Reinforcement Learning for Mention Recommendation**

7

8 HANCHI HUANG, Shanghai Jiao Tong University

9 YAN LI,Shenzhen Polytechnic

10 LI SHEN, JD Explore Academy

11 JIANGHAO LIN,

12 Shanghai Jiao Tong University

13 XIN XIN,Shandong University

14 In social networking platforms such as Twitter, users tend to mention other related users (indicated by the "@" symbol) when

15 posting messages. In these scenarios, automatically suggesting candidate users who are likely to be mentioned can improve

16 communication efficiency and the user experience. Existing work typically uses recent posts or a few randomly selected

17 historical messages to infer user preferences and relationships between them. However, few papers have taken into account two

18 fundamental properties of mention recommendations during this inferring process, namely dynamic cooperation among users

19 and the sparsity of their interactions. To address these issues, we propose an MA-DGNN algorithm, a graph-based multi-agent

20 reinforcement learning (MARL) approach that consists of two key components: a time-delayed aggregation graph policy network

21 and a custom prioritized experience replay (PER). As to the time-delayed aggregation graph policy network, the time-delayed

22 graph support operation is applied to act as a distributed controller for large networks of users with interaction dynamics and

sparse communication, where a time-delayed multi-hop information diffusion mechanism helps sparsely interacting users to

23 obtain advice from densely interacting users. As to the prioritized experience replay mechanism, it combines six carefully

24 chosen metrics to handle sample imbalance and non-stationarity of user preferences and to leverage statistical patterns in user

25 interactions. Moreover, regarding the inferring process, MA-DGNN updates the user-user utility matrix by decomposing the

26 neural matrix based on similarity loss to aggregate the output of the policy network and aid in credit assignment in MARL;

27 regarding the loss function, the loss functions of MA-DGNN’s policy and critic networks are modeled as minimal maximum

28 objectives to learn robust policies in the face of multiple non-stationarity from both the environment and other agents. Extensive

29 experiments on synthetic and real datasets demonstrate that our MA-GDNN outperforms the existing state-of-the-art approaches.

30 CCS Concepts: • Computing methodologies → Artificialintelligence.

31

32 Additional Key Words and Phrases: Mention Recommendation, Multi-agent Reinforcement Learning, Graph Neural Network

33 ACM Reference Format:

34 Hanchi Huang, Yan Li, Li Shen, Jianghao Lin, and Xin Xin. 2022. Multi-agent Reinforcement Learning for Mention Recom-

35 mendation. Proc. ACM Meas. Anal. Comput. Syst.37, 4, Article 111 (August 2022),19 pages.[ https://doi.org/10.1145/nnnnnnn.](https://doi.org/10.1145/nnnnnnn.nnnnnnn)

36 [nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)

37

38 1 INTRODUCTION

39 Mention notification is a very popular tool when people communicate on social networks. By tagging an ’@’

40 symbol with a username concatenated in the post message, a ’You have been mentioned’ notificationwould be

41 pushed to the specificuser. Automatically suggesting who is the best to be mentioned with an ’@’ is a key problem

42 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.002.png)

43 Authors’Polytechnic;addresses:Li Shen,Hanchimathshenli@gmail.com,Huang, qq20001224m@sjtu.edu.cn,JD Explore Academy;ShanghaiJianghaoJiaoLin,Tongchiangel@sjtu.edu.cn,University; Yan Li, Shanghaiyb57411@szpt.edu.cn,Jiao Tong University;ShenzhenXin

44 Xin, xinxin@sdu.edu.cn, Shandong University.

45 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.003.png)

46 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are

47 not made or distributed for profitor commercial advantage and that copies bear this notice and the full citation on the firstpage. Copyrights for

48 components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to

post on servers or to redistribute to lists, requires prior specificpermission and/or a fee. Request permissions from permissions@acm.org.

49 © 2022 Association for Computing Machinery.

50 2476-1249/2022/8-ART111 $15.00

51 <https://doi.org/10.1145/nnnnnnn.nnnnnnn>

61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55 56 57 118 59

61

5 6 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.004.png)7 8 

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

24 Fig. 1. Mention recommendation processed by our proposed MA-DGNN.

25

26

27 in social networks titled as mention recommendation. It brings in many benefitsincluding but not limited to (i) 28 strengthening the relationship with friends in that the one mentioned feels a special greeting and care from the user 29 who publishes the tweet [27]. The reactions make friends closely connected and spend more time and attention on 30 each other. (ii) bridging over different circles, since the issued post could be forwarded to friends of friends more 31 than we can imagine. It is similar to a view from [14] that the tagging activity promotes the crossover of social 32 graphs on Twitter, which is widely applied in commercial promotion. Breaking the boundaries and touching the 33 potential customers are just what the retailers want most; and (iii) improving reciprocity over a social network. 34 Take LinkedIn for instance, the published post could be kept hot on the LinkedIn platform if the publisher mentions 35 the right persons with great interest as well as great public impact. Similar examples on Facebook could be found 36 in [10, 27].

37 The whole process of mention recommendation is displayed in Figure1. For each recommendation round, in the 38 beginning, a user colored pink issues a post and intends to ’@’ somebody. When the user inputs the ’@’ symbol, 39 the algorithm (named MA-DGNN in Figure 1) should quickly recommend a list of candidate mentionees (with 40 colors blue, green, yellow, and purple, etc.) for reference. Since the pink user has the biggest preference for the 41 blue user, the pink user mentions the blue user ultimately. After receiving the ’mention’ notification from the 42 pink user, the blue user finds the post from the pink user very interesting and continues to share the post with 43 other friends according to his social network. Therefore, by the information diffusion on a social network, the 44 relationship between users is continuously evolving, which exerts influenceon not only the algorithm’s follow- 45 up recommendation but the subsequent users’ ’@’ decision. Since mention recommendation is a multi-round 46 recommendation process involving the complex and dynamic relationship among the post publisher, the post’s 47 content, and other users on social networks, it seems more natural to use reinforcement learning (RL) instead of 48 supervised learning which assumes environments’ stationarity, and to use multiple agents instead of a single agent 49 to model evolving interactions between users while making personalized decisions for each user.

50 Existing studies on mention recommendation could be mainly categorized into three methods: ranking-based 51 methods [17, 32, 34, 45], classification-basedmethods [5, 16] and Multi-agent Reinforcement Learning (MARL)

ACM Transactions on Recommender Systems Page 61 of 20

61 61 62

4 Multi-agent Reinforcement Learning for Mention Recommendation • 111:3

5

6 based methods [9]1. For ranking-based methods, the target user would be recommended with a ranked list of 7 potential mentionees achieved by a global scoring function. Similarly, classification-basedmethods address mention 8 recommendation from a classificationperspective, i.e., potential mentionees should be either predicted as positive 9 or negative in each mention behavior towards the target user. However, these two methods fail to consider the 10 following two issues. Firstly, the global scoring function or classification model, as a unified strategy for all 11 users, could not well capture the very different styles of different users and their preferences evolving in different 12 directions as time goes by. Secondly, the aforementioned methods generally suffer from the sparsity problem, i.e., 13 lack of interaction signals between users. Ideally, different levels of relationships between different users and other 14 related information should be fully leveraged to make better recommendation decisions for users with less historical 15 interaction records.

16 To address these challenges, we propose MA-DGNN, a graph-based MARL model with deliberately customized 17 prioritized experience replay to solve mention recommendation tasks with a large number of users. Each user on 18 the social network would be treated as an agent and the interactions with other users would be well modeled as 19 cooperations with the environment. The system is expected to automatically generate highly qualifiedmentionee 20 candidates such that the target user would prefer to hit them with the symbol ’@’. First, during state embedding, we 21 employ an LSTM-based graph neural network to model users’ complex time-varying dynamics, since the messages 22 along with user interactions diffuse over the social network in a sequential manner. To handle a large number 23 of users and the sparsity and non-stationarity of interactions between users, we adopt the delayed aggregation 24 graph neural network (DGNN) [33] as the policy network of MARL, which combines the time-delayed graph 25 operation of different orders to serve as distributed controllers for large networks of users with interacting dynamics 26 and sparsely available communications. Thanks to different levels of information diffusion along the multi-hop 27 neighbors in DGNN, the relationship between nodes with sparse interactions might be well inferred from the 28 flowing information among other nodes with varying distances. Last but not least, a customized personalized replay 29 buffer (PER) with non-stationarity detection and state clustering techniques is also utilized to make full use of 30 historical statistics and handle imbalanced datasets as well as sparse user interactions.

31 Below, we summarize our main contributions as four-fold:

32 • We are the firstones to model each user as an agent in mention recommendation problem and the firstone to 33 introduce a graph-based MARL approach with time-delayed graph support which models the sparse and dynamic 34 interactions among users to solve the problem with a large number of users. A customized PER mechanism and a 35 reward reshaping technique are designed elaborately to handle the sample imbalance and the non-stationarity of 36 users’ preferences, and take full advantage of the statistical laws inside users’ interactions.

37 • During the training of MARL, we customize minimax objectives to minimize the losses of the policy and critic 38 networks for a worst-case scenario, so as to learn robust policies in the face of multiple non-stationarity from 39 both environments and other agents. The minimax optimization technique, the customized PER mechanism, and 40 the LSTM-based graph neural network for state embedding together empower our algorithm with great adaptivity 41 to the changes of both tweets’ styles and users’ interactions.

42 • To make a comprehensive aggregation for the outputs of DGNN as well as performing credit assignment, we 43 construct an agent-agent utility matrix which learns the contribution of each user’s suggestion for other users 44 and regard each row of the matrix as the aggregation coefficients. The neural matrix factorization technique, 45 which can well deal with data sparsity by learning hidden factors, is novelly proposed to optimize the matrix. An 46 ablation study is performed to successfully show the great performance improvement when using this decision 47 aggregation mechanism.

48

49 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.005.png)

50 1Although Gui et al. [9] propose a MARL method to tackle mention recommendation problems, the method only constructs two agents, i.e., two 51 tweet selectors with different functions instead of modeling each user as an individual agent, and thus neglects users’ personal preference.

61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56

57

58

59 118

5

6 • Extensive experiments on the synthetic and real-world datasets with six classic evaluation metrics demonstrate 7 the absolute performance superiority of our algorithm over existing methods.

8

9 2 RELATED WORK

10

11 2.1 Mention Recommendation

12 There are a bunch of studies proposed to address mention recommendation. The most popular employed methods 13 are ranking-based models and classification-basedmodels. For ranking-based methods, a global ranking function 14 could be achieved by (semi-) supervised learning. Specifically, Wang et al. [34] adopt ranking support vector 15 regression with features respecting user interest match, user relationship, and user influence.Tang et al. [32] apply 16 the ranking support vector machine with features regarding content, social, location, and time. Wang et al. [35] 17 introduce a semi-supervised deep model to capture the highly non-linear relationship between nodes.

18 For classification-basedmodels, whether to recommend each user is deduced by tackling several classification 19 problems. Pramanik et al. [23] introduce a tweet propagation model based on a multiplex network framework 20 to handle the classification problems, which allows analyzing the effects of mentioning on final retweet count. 21 Wang et al. [36] propose a generative model to solve mention recommendation by learning users’ semantic patterns 22 and the correlation between users’ multi-modal mention documents in a unified way. Yi et al. [39] construct a 23 heterogeneous mention network to model different entities such as authors, messages, and users into a unified 24 low-dimensional embedding vector space, such that the heterogeneous information can be calculated in the same 25 embedding space.

26 However, the above approaches and the MARL method in [9] take a unifiedstrategy to serve for all users which 27 might neglect users’ personalized preferences. Besides, these methods do not fully characterize the two primary 28 properties of mention recommendation, i.e., the dynamic and sparse communications of user behaviors, which 29 make them not quite viable in our problem setting.

30

31 2.2 Reinforcement Learning for Recommendation

32

33 Reinforcement learning (RL) is the process of agent learning to maximize the long-term rewards when interacting 34 with the environment [31]. It has been widely introduced into recommendation systems due to its consideration of 35 users’ long-term feedback. Via interacting with a target user, the recommendation platform would learn her item 36 preference step by step according to the reward, e.g., the user’s click or longer dwelling time. However, such a 37 scenario is quite different from our problem setting. In social networks, the node (user) and edge (relationship) 38 adding or disappearing happen all the time. The interaction environment becomes more complex than the general 39 recommendation. Despite this huge difference, we would still introduce the existing RL-based methods over general 40 recommendation as follows. Existing RL methods for recommendation can be roughly divided into two categories, 41 value-based methods [6, 38, 41, 43, 44] and policy-based methods [4, 11, 12, 19, 26, 28, 37, 40, 42].

42 Value-based methods evaluate the Q value for each candidate item (or item set) and select the item (or item set) 43 with the highest Q-value. Recent literature approximates Q-values mostly via Deep Q-Networks. For example, 44 Zheng et al. [44] propose a Deep Q-Learning based recommendation framework with the user return pattern as 45 a supplement to user feedback. Similarly, Zhao et al. [43] leverage the Deep Q-Network and integrate skipped 46 items (negative feedback) into RL-based recommendation methods. Nevertheless, it can be computation inefficient

for value-based methods to evaluate Q-values for all items when the item space is very huge. As a consequence, 47

some literature adopts policy-based methods. They explicitly construct a representation of the policy, which maps 48

environment state to action and store it in memory during the learning process. For instance, Zhao et al. [42] 49

propose a page-wise recommendation framework that leverages Deep Deterministic Policy Gradient (DDPG) to 50

automatically learn the optimal strategies. Meanwhile, Hu et al. [12] develop a policy gradient algorithm to learn 51

ACM Transactions on Recommender Systems Page 6 of 20

61 61 61

Multi-agent Reinforcement Learning for Mention Recommendation • 111:5

4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

Table 1. Notations.



|Notation|Meaning|
| - | - |
|<p>T</p><p>t</p><p>N 𝑡𝑚𝑡</p><p>𝑜𝑡 𝑡ℎ�𝑡</p><p>𝑥𝑗 𝑦𝑗 𝑝𝑡 𝑛𝑡</p><p>𝑟𝑡,𝑖 𝑆𝑛</p>|<p>total number of rounds</p><p>current time</p><p>number of users</p><p>message at time t</p><p>linearly weighted value output by DGNN and utility matrix</p><p>�𝑒threshold which decides whether to recommend the most recent K users that user𝑖𝑡has mentioned or to randomly select K users except user𝑖𝑡to user��</p><p>the feature vector of the 𝑗-th latest tweet that user 𝑗has issued</p><p>the feature vector of the 𝑗-th latest user that user 𝑗has mentioned</p><p>the number of historical positive samples till time𝑡</p><p>the number of historical negative samples till time𝑡</p><p>the reward of agent𝑖∈ [𝑁] at round𝑡</p><p>a graph shift operator that describes the graph support𝐺𝑛</p>|
an optimal ranking policy, as well as dealing with the high reward variance and unbalanced reward distribution in the search session Markov decision process.

However, seldom do existing RL strategies consider tackling the mention recommendation problem, where the interactions between the target user with the others are affected by the complex time-varying dynamics and the interaction sparsity makes the learning process difficult to address. Although the proposed RL-based strategies

24 share a similar long-term rewards maximization goal, they are actually not feasible for our problem2.

25

26 3 METHODOLOGY

27 In this part, we firstformulate the mention recommendation task in Section3.1 and then present the overview and 28 core components of our proposed method MA-DGNN in Section 3.2.

29

30 3.1 Problem Formulation

31 Assume there are 𝑁users in a social network platform. At round𝑡∈ [𝑇], user𝑖𝑡comes into the system and issues 32 a message𝑡𝑚𝑡, e.g., a tweet or microblog. Given𝑖𝑡and𝑡𝑚𝑡, the system must recommend one or several users from 33 a list of candidate mentioned users[𝑁] 3 according to their long-term historical tweetsH . The objective is to have 34 an overall great performance with regard to precision, recall, and F1-score, the three popular evaluation metrics in 35 mention recommendation tasks and traditional recommendation system field.

36

37 3.2 Algorithm

38 Figure 2 shows the architecture of our proposed method MA-DGNN. To model the dynamic relationship between 39 users and make a personalized recommendation for each user, we regard each user as a node in a graph, as well 40 as an agent in the multi-agent reinforcement learning (MARL) system. At round 𝑡∈ [𝑇], a delayed aggregation 41 graph neural network (DGNN) is utilized to output a real value for each node/user, which applies the delayed graph 42 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.006.png)

43 2couldTo achiemodelve personalizedthe personalizedmentionstraterecommendationgy for each user asandeachbetterindiutilizationvidual agentonandusers’inferinteractions,relationshipit amongseems promisingusers by learningto utilizeagents’MARLdynamicwhich 44 and cooperative behaviors. Although MARL-based methods could adopt personalized strategies and help alleviate the sparsity problem, they are

45 still in face of specificlimitations to be overcome. Firstly, samples from MARL’s replay buffer might be out of date due to users’ dynamic 46 preference, and the mixture of noisy samples from various users would cause a great interference to policy updating and inferring process. 47 Secondly, when updating a policy for a certain agent, other agents could be regarded as environments and the non-stationarity of other agents

(i.e., environments) breaks the Markov assumption of single-agent RL, which might result in performance degradation or even failures of policy 48 learning. To handle these challenges, we equip our MA-DGNN with a series of novel techniques which demonstrate great performances in the

49 experimental session.

50 3In experiments we recommend the candidate mentionees from the whole user set [𝑁] for each target user because the times that MA-DGNN 51 recommends user 𝑖𝑡when user 𝑖𝑡is issuing a message is too rare to influencethe ultimate performances.

61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55 56 57 58 59 61

5

6

7

8 Tweet User i\_t Mehintistooneryd 

**Round t SeqConv: GCN-LSTM**

9 **Static state Dynamic state**

10 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.007.png)

11 **Decentralized actor Centralized critic ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.008.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.009.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.010.png)**

12 **User-user utility matrix** Composite loss ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.011.jpeg)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.012.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.013.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.014.png)

13 Update by neural matrix factorization: **Delayed Aggregation Credit Assignment ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.015.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.015.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.016.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.017.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.018.png)**14 loss 1-scaled cross-entropy Agent 1 **GNN** Agent N![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.019.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.015.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.015.png)

15 Agent 2 ...... Agent N-1 **replay![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.020.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.021.png)**

**Prioritized experience**

loss 2-similarity-based term

16 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.022.png) ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.010.png) ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.023.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.014.png) Non-stationary detection ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.009.png)17 Ultimate**＋** decision

18 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.024.jpeg)

19 **User feedback**

20

21

22

Fig. 2. The overall framework of MA-DGNN.

23

24

25 support operation to serve as distributed controllers for large networks of users with interacting dynamics and 26 sparsely available communications. Then we use the 𝑖𝑡-th row of a user-user utility matrix to aggregate the real 27 values for every user and output a linearly weighted value𝑜𝑡∈ [0,1] to decide which users should be recommended. 28 The utility matrix models the dynamic relationship between each pair of users and is updated by a neural matrix 29 factorization method with the loss considering the similarity between users. It is optimized independently to DGNN 30 instead of being jointly and thus slowly updated with DGNN via only one feedback at each round. After obtaining 31 𝑜𝑡, we compare it to a threshold𝑡ℎ𝑟𝑒𝑡4. If𝑜𝑡> 𝑡ℎ𝑟𝑒𝑡, then we recommend the most recentK users that user𝑖𝑡has 32 mentioned; otherwise, K users except user 𝑖𝑡are randomly selected for the recommendation to user 𝑖𝑡.

33 We use a unifiedcritic network and decentralized actor networks in MARL with minimax optimization for robust 34 policy learning and adopt a customized experience replay mechanism with state clustering and non-stationarity 35 detection to update both the policy network and the critic network, which consider both the sample diversity and 36 users’ varying preferences. As for the critic loss, we perform credit assignment to the 𝑁agents with weights 37 being the 𝑖𝑡-th row of the user-user utility matrix, which is proven to obviously improve the performance of our 38 MA-DGNN in the ablation study part. Lastly, to handle the seriously imbalanced data, we scale rewards in different 39 scenarios and the different terms in the loss of neural matrix factorization according to the relative ratio between 40 positive samples and negative samples5.

41 Below, we give a detailed description of each module.

42

43 3.2.1 The composition of Markov Decision Process. We model the long-term mention recommendation 44 process as a Markov Decision Process (MDP) and introduce the state, action, and reward configuration in the MDP 45 in detail.

46 State. We regard each user as an agent in MARL. For𝑖∈ [𝑁], the state of agent𝑖consists of two parts: the static 47 and dynamic parts.

48 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.025.png)

49 4𝑡ℎ𝑟𝑒𝑡is a sliding-window threshold which will be described later.

50 5Positive samples refer to samples in the rounds where the mentionee belongs to the most recent K users that the currently served user has 51 mentioned. The negative samples are samples in the remaining rounds.

ACM Transactions on Recommender Systems Page 8 of 20

61

61

61

61 Multi-agent Reinforcement Learning for Mention Recommendation • 111:7

5

6 • The static part is the pre-trained feature vector of user𝑖, which shows the user’s inherent features such as birth,

7 birthplace, education, etc.

8 • With regard to thedynamicpart, since the historically issued tweet list and the mentionee list of a user are essential

9 to the user’s characterization, we also consider SeqConv [? ], an LSTM-based graph convolutional operator,

10 which is useful for graph datasets where each node represents a sequence, to deal with the two aforementioned

11 time series and form the dynamic part of a user’s state. Specifically, the dynamic part of user (agent) 𝑖’s state is

12 𝑆𝑒𝑞𝐶𝑜𝑛𝑣([𝑥1,𝑦1],[𝑥2,𝑦2],···, [𝑥𝑁 ,𝑦𝑁 ]), where𝑥𝑗(𝑗∈ [𝑁𝑙𝑖𝑠𝑡]) refers to the feature vector of the𝑗-th latest

13 tweet that user 𝑖has issued, and 𝑦𝑗𝑙( 𝑖𝑠𝑗 𝑡∈ [𝑁𝑙𝑖𝑠𝑙 𝑡𝑖𝑠𝑡]) refers to the feature vector of the 𝑗-th latest user that user 𝑖has

14 mentioned. In the beginning, when the system has not collected enough samples, the uncollected 𝑥𝑗and𝑦𝑗are

15 set to be the zero vectors.

16 The state input to the policy network and the critic network as stated below is the concatenation of all agents’ states.

17 Action. The action is the concatenation of the𝑁real values output by the graph-based policy network, as stated

18 below. The ultimate decision, however, depends on whether𝑜, the linear combination of the𝑁real values, exceeds

19 a sliding-window threshold 𝑡ℎ𝑟𝑒𝑡, the 𝑛𝑡+1 percentile of the𝑡list {𝑜𝑖} , where 𝑝𝑡is the number of historical

21 positialgorithmve sampleswill recommendand𝑛𝑡is thethenumbermost recentof𝑝𝑡historical+1K usersnethatgatithevecurrentlysamples. Ifserv𝑖∈yes,[𝑡−ed1]thenuserthehasultimatementioneddecisionto thatisuser1, i.e.,. If not,the

20

22 then the ultimate decision is0, i.e., the algorithm will randomly selectK users for recommendation to the currently

23 served user. As can be seen in the ablation study, setting 𝑡ℎ𝑟𝑒to be 𝑡~~ , which considers datasets’ imbalanced

24 property, instead of the normal threshold 0.5 improves the performance𝑡 𝑛𝑝𝑡++of11 our MA-DGNN remarkably in all real

25 datasets.

26 Reward. To update each node of DGNN according to their different contributions for the ultimate decision, we

27 customize different rewards for each node at each round. Specifically, at each round 𝑡, our algorithm will output

28 a real value 𝑜0 ∈ [0,1] for each user 𝑖∈ [𝑁] and we thus sample a binary value 𝑜1 ∼ 𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖(𝑜0 ). If the 29 𝑡,𝑖 𝑡,𝑖 𝑡,𝑖

mentioned user at round𝑡was mentioned by user𝑖𝑡in user𝑖𝑡’s most recentK rounds, then we say that the behavior

30 at round𝑡is a positive sample and the ground truth at round𝑡is 1; otherwise, the behavior at round𝑡is a negative

31 sample and the ground truth is 0. Since in real datasets, the number of negative samples far outweighs the number

32 of positive samples, we scale the terms in rewards to handle the issue of imbalanced datasets. Specifically, first,

33 denote the ground truth at round 𝑡to be𝐺𝑡. Then, for the reward of agent 𝑖∈ [𝑁] at round 𝑡, we set

34 𝑝![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.026.png)

35  1− 1+𝑛𝑡+𝑝, if𝑜𝑡1 ,𝑖= 𝐺𝑡= 0;

36 −1, if𝑜𝑡𝑡1 ,𝑖𝑡= 0,𝐺= 1;

38 𝑟𝑡,𝑖=  1, if𝑜11+=𝑛𝑡+𝐺𝑝𝑡𝑡)=, 𝑡if1.𝑜𝑡1 ,𝑖= 1,𝐺= 0; (1)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.027.png)

37 −(1− 𝑝𝑡 𝑡



- 𝑡,𝑖

39 The design of𝑟is to give a bigger reward or punishment for the prediction of the rarer positive samples. Otherwise,

40 if𝑟∈ {−1,1}𝑡,for𝑖all𝑡and𝑖, great chances are that the learning of rarer positive samples will be quickly offset by

41 the𝑡major,𝑖 negative samples and the performance of MA-DGNN might hurt much accordingly.

42

43 3.2.2 Our customized cooperative MARL algorithm. We regard each user as an agent and design a graph-

44 based MARL algorithm to make a comprehensive decision for each user. The minimax multi-agent deep determin-

45 istic policy gradient (M3DDPG) approach [18] is adopted to update our proposed algorithm, for the purpose of

46 learning robust policies. Below we illustrate the structure of our method in detail.

47 Centralized critic and decentralized actors with minimax optimization. We take a centralized critic which

48 absorbs the concatenation of all agents’ states and actions (decentralized actors). The critic, then, outputs the value

49 vector which evaluates the state-action pair of each agent, so as to update the policy network.

50 Since during the training of MARL, agents’ policies might be very sensitive to the changes in other agents’

51 strategies, which could slow down the speed of policy loss’s convergence as well as make the decision-making

61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56

57

58

59

61

5

6 process out-of-order. In order to learn more robust policies, we are the firstones in the fieldof mention recommen- 7 dation who follow [18] to update the policy and critic networks considering the worst situation, i.e. optimizing

8 the reward of each agent while assuming other agents perform adversarially. Formally, we customize minimax

9 objectives for each agent to update the policy and critic networks at round 𝑡, where minimax is a fundamental 10 concept in game theory and can be applied to general decision-making under uncertainty, prescribing a strategy 11 that minimizes the possible losses for a worst-case scenario, as stated in [18]. Specifically, the minimax objectives 12 of the policy network and the critic network for agent 𝑖are

13 𝝁 0

14 max𝝁 min0 𝑄𝑖(s𝑡,𝑎1,...,𝑎𝑖,...,𝑎0𝑁)|𝑎𝑖=𝝁𝑖(s𝑡), (2)

𝑎𝑗≠𝑖

15 and

16 max − L = − 𝑄𝝁s ,𝑎,...,𝑎 − 𝑦2,

17 𝑡,𝑖 𝑖 𝑡𝑡,1 𝑡,𝑁 𝑖

18 𝑦𝑖= 𝑟+𝛾𝑄𝝁′ s𝑡+1 ′★ ′

20 𝑎𝑖′ = 𝝁𝑡𝑖′ ,𝑖(s𝑡+1)𝑖, ,𝑎1 ,...,𝑎𝑖,...,𝑎′𝑁★ , (3) 19

21 𝑎′★ = argmin 𝑄𝝁′ s𝑡+1

22 𝑗≠𝑖 𝑎𝑗≠𝑖 𝑖 ,𝑎1′,...,𝑎′𝑁,

′

23 respectively. In (2) and (3), s𝑡is the concatenation of states for all agents at round 𝑡, 𝑎𝑡,𝑖(𝑖∈ [𝑁]) is the action of 24 agent 𝑖at round 𝑡, 𝝁is the policy of all agents, 𝝁(st) is the 𝑖-th component of 𝝁(st), and 𝝁′ is the target policy

25 netwToorkderiofveM3DDPG𝑎0 and 𝑎[18′★]. 𝑖

26 𝑗≠𝑖 𝑗≠𝑖for each agent 𝑖∈ [𝑁], we adopt the end-to-end solution that approximates the 𝑄 27 function by a locally linear function and replaces the inner′★-loop minimization in(2) and (3) with a 1-step gradient 28 0𝑗≠𝑖 𝑗≠𝑖

descent. Below is the calculation procedure of 𝑎 and 𝑎 :

31 𝑎0 = 𝑎𝑗𝑘+𝜖0𝑗, ∀∀𝑗1≠≤𝑖,𝑘≤ 𝑁,

29 𝑎𝑘= 𝝁(s𝑡),

30

𝑗

32 𝜖0𝑗≠𝑖= argmin 𝑄𝑖𝜇s ,𝑎+𝜖,...,𝑎𝑖,...,𝑎𝑁+𝜖0

33 𝜖0𝑗≠𝑖 𝑡1 10 𝑁

34 ≈ −𝛼𝑗∇𝑎𝑄𝑖𝝁(s𝑡,𝑎1,...,𝑎𝑁) .

𝑗

35 𝑎′ = 𝝁′ (s𝑡+1), ∀1 ≤ 𝑘≤ 𝑁, (4) 36 𝑎𝑘′★ = 𝑎𝑘′ +𝜖′, ∀𝑗≠ 𝑖,

37 𝑗 𝑗 𝑗

41 𝑗≠𝑖≈ −𝛼𝜖𝑗′𝑗∇≠𝑖𝑎′𝑗𝑄𝜇𝑖𝝁′𝝁′ 1 𝑖′ ′ ′

38 𝜖′ = argmin 𝑄𝑖s𝑡+1,𝑎1′ +𝜖′,...,𝑎,...,𝑎𝑁+𝜖𝑁

39

43 𝑖∈ [𝑁], we linearly combine the policy losses𝑗≠𝑖for alls𝑡agents1 1′ and the 0critic𝑎𝑖losses(s𝑡) for all agents, respecti𝑡,v 𝑖ely, with 40 𝑡+1,𝑎,...,𝑎′ .

𝑁

42 After obtaining the policy loss, i.e., − min𝑎0 𝑄𝑖(s ,𝑎0,...,𝑎𝑖,...,𝑎𝑁| =𝝁 ), and the critic loss L for each

𝑖

44 the combination coefficientsdescribed in the below credit assignment part.

45 Delayed aggregation graph policy network. To model the dynamic and sparse communications between users 46 and make a comprehensive decision, we adopt the delayed aggregation graph neural network (DGNN) [33] as the 47 policy network of MARL. This method was never used in recommendation systems and we novelly integrate it into 48 our elaborately designed MA-DGNN which results in great experimental performances on various datasets. Thanks 49 to different levels of information diffusion along the multi-hop neighbors in DGNN, the relationship between agents 50 with sparse interactions might be well inferred from the flowing information among other agents with varying 51 distances. Below, we introduce DGNN in detail.

ACM Transactions on Recommender Systems Page 10 of 20

61

61

61

61 Multi-agent Reinforcement Learning for Mention Recommendation • 111:9

5

13 mannerGNNstime-vthenAggreUsedescribearyingthat,(𝑖utilizingg ,ation𝑗)deal∈graphtheEGNNswithusefulgraphtoprocessesfixrepresent[7edinformation]supportaregraphinformationsetthatsignals𝐺upthrough𝑗byomightveradefinedgraphtime-vprocessingrepeatedsendshiftoaryingvdataerframefixeoperatorxchangestographed𝑖graphwatorkstimesupport.𝑆withsupport,which∈𝑛mightR. neighbors.WithoperatenotTolstayathesebe Difonzerodenotations,etnetwferent𝑛al.only𝑖𝑥𝑗(ork[𝑛might33−from1if])datae(𝑗xtend𝑗Tnormal,not= 𝑖olstayain)𝑥∈⊤beathemEdecentralizedzeroaggreorettotoal.ifonlygbetackle𝑖ation[=33theif𝑗], 6

7

8

9

10 𝑛

11 𝑛 𝑛 𝑁×𝑁where [𝑆]

12 (𝑗,𝑖) ∈ E𝑛or if 𝑖= 𝑗, indicating the sparsity of the graph. More specifically, denote 𝑗(𝑛−1)

19 westateobtain:of node 𝑗at time 𝑛− 1,[then𝑆𝑛𝑥𝑛−through1] =𝑗𝑗𝑘=(−𝑛𝑖∑︁,1− 𝑗the∈1)N𝑖𝑛property[𝑆𝑛]𝑖𝑗[𝑥that𝑛−1𝑖] 𝑛[𝑆=𝐾]−1 𝑥𝑗(𝑛−𝑘) 𝑖𝑛 𝑛

𝑛𝑖𝑗

14

15 𝑖 𝑗,Nin={𝑗: (𝑖, 𝑗) ∈ E𝑛}. (5) 16

17

18 N 𝑘= 𝑗′ ∈N : 𝑗∈ N𝑖𝑛,H : 𝑗∈N𝑘. (6)

𝑖𝑛

𝑘=0

20 Based on the locality of (5), delayed aggregation GNNs build a sequence of recursive 𝑘-hop neighborhood 21 aggregations as shown in (6), (7), and (8). Specifically speaking, define a sequence of signals 𝑦𝑘𝑛∈R𝑁×𝑝with 22 𝑦0𝑛= 𝑥𝑛and

23 𝑦𝑘𝑛= 𝑆𝑛𝑦(𝑘−1)(𝑛−1). (7) 24 Then it holds that𝑦𝑘𝑛= (𝑆𝑛𝑆𝑛−1...𝑆𝑛−𝑘+1)𝑥𝑛−𝑘. Therefore, (7) is characterizing the diffusion of𝑥𝑛−𝑘through the 25 series of time varying networks from𝑆𝑛−𝑘+1 to 𝑆𝑛. Next, aggregate𝑦𝑘𝑛for𝑘∈ {0,1,···,𝐾− 1}to form the nested 26 states along the multi-hop neighbors:

27 Z 𝑖𝑛= [𝑦0𝑛]𝑖;[𝑦1𝑛]𝑖;...; 𝑦(𝐾−1)𝑛𝑖, (8) 28

29 where the 𝑘+ 1-st element of 𝑧𝑖𝑛, [𝑦𝑘𝑛]𝑖= [𝑆𝑛𝑆𝑛−1...𝑆𝑛−𝑘+1𝑥𝑛−𝑘]𝑖, is an average of 𝑥𝑗(𝑛−𝑘) of 𝑘-hop neighbors 30 𝑗∈ N𝑘at time 𝑛− 𝑘.

𝑖𝑛

31 Specifying that 𝑧𝑖𝑛has a regular temporal structure due to its nested aggregation property, Tolstaya et al. [ 33] 32 model 𝑧𝑖𝑛by a convolutional neural network with length 𝐿, that is, for𝑙∈ [𝐿], set

33 Z (ℓ) = 𝜎(ℓ) H(ℓ)Z (ℓ−1) (9) 34 𝑖𝑛 𝑖𝑛

35 where 𝜎(ℓ) is a nonlinearity operator and H(ℓ) is a bank of small-support filters shared across all nodes with 36 learnable parameters.

37 To sum up, the delayed aggregation GNN architecture, characterized by(5)-(9), comprises a local parameteriza- 38 tion of the policy𝜋(H𝑖𝑛,H) which captures the dynamic and sparse interactions in a network and allows for distant 39 communications through the multi-hop information diffusion.

40 Customized experience replay. To (i) absorb experience from users with larger similarity to other users and 41 better decision quality for other users, and (ii) attach greater importance to the newer samples and the rarer positive 42 samples, during the sampling process in MARL’s replay buffer, we are the first one to take all of the following

43 factors into account: (i) the time when a sample occurs; (ii) whether the user feedback is positive or negative; 44 (iii) the times that the target user in a sample mentions a person; (iv) the times that the target user is mentioned

45 by other users; (v) the averaged performance that the target user assists on other users’ decisions6; and (vi) the

46 averaged similarity between the target user’s state and other users’ states. We calculate the aforementioned six

47 factors𝑧𝑖0 ,1,𝑧𝑖0 ,2,···,𝑧𝑖,6 for each sample𝑖∈ [𝑁𝑏𝑢𝑓 𝑓𝑒𝑟]. With regard to each factor 𝑗∈ [6], we perform the softmax

0

48 function on 𝑧0 ,𝑧0 ,···,𝑧0 and obtain 𝑧1,𝑗,𝑧2,𝑗,···,𝑧𝑁 ,𝑗. Then for each sampling process, for the 𝑖’s 49 1,𝑗2,𝑗 𝑁𝑏𝑢𝑓 𝑓 𝑒𝑟,𝑗 𝑏𝑢𝑓 𝑓 𝑒𝑟![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.028.png)

50 6Assume user𝑖is the target user of a certain sample in replay buffer, and𝑗is the time when the sample occurs. Then, the averaged performance

𝑗

51 that the target user assists on other users’ decisions is characterized by ( 𝑘=1𝑟𝑘,𝑖)/ 𝑗.

61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56

57

58 59 61

5

6 sample (𝑖∈ [𝑁 ]) in the replay buffer, the corresponding sampling probability is 6𝑗=1𝑧𝑖,𝑗. Note that to better 7 approximate most𝑏𝑢𝑓 𝑓of 𝑒𝑟the actions and the state-action pairs, it is necessary to update MA-DGNN6 with a rich variety

8 of samples. However, since samples provided by the above method might demonstrate a strong tendency regarding 9 the six indexes, the diversity of the training samples could be far from enough. Therefore, to boost the diversity 10 of experience replay, we take the above sampling method for a part of the training samples and the other part is 11 obtained by state clustering. Specifically, we cluster the samples in the buffer into10groups according to their state 12 and select samples from each group uniformly at random with an equal amount. Furthermore, during the sampling 13 process, we novelly remove a certain amount of samples according to the degree of samples’ non-stationarity. 14 Specifically, we measure the standard error of the true labels for samples in each group and obtain the averaged 15 value, 𝑠𝑡𝑑𝑎𝑣𝑔,𝑡, of the 10 standard errors. If 𝑠𝑡𝑑𝑎𝑣𝑔,𝑡is larger than 80%of the historical {𝑠𝑡𝑑𝑎𝑣𝑔,𝑖}𝑖∈[𝑡−1], then we 16 judge that there might exist obvious non-stationarity and thus remove the oldestmin{100,𝑠𝑡𝑑𝑎𝑣𝑔,𝑡/100}percent of 17 samples to keep track of the latest preferences of users.

18 Credit assignment. Recall that s𝑡is the concatenation of states for all agents at round 𝑡, 𝑎𝑡,𝑖(𝑖∈ [𝑁]) is the 20 action of agent𝑖, and 𝝁is the policy of all agents. For each agent𝑖∈ [𝑁], the policy loss is𝑄𝑖𝝁(s𝑡,𝑎𝑡) ln𝜋(𝑎𝑡,𝑖|s𝑡),

19

where 𝜋is the probability of obtaining𝑎under states and policy 𝝁. In the critic network and the policy network, 21 we assign different weights to the critic𝑡loss,𝑖and the polic𝑡y loss of different agents and sum the losses to perform

22 unifiedback-propagation. The weight coefficientsare the elements of the 𝑖-th row of the user-user utility matrix, 23 just as stated below.

24

25 3.2.3 Design and update of user-user utility matrix. To make a comprehensive decision and perform the 26 credit assignment in MARL, we construct a user-user utility matrix to obtain the weights of linear combinations. 27 Since there are 𝑁2 parameters in the utility matrix which require much time to be correctly approximated, we 28 assume the utility matrix can be factorized into two small matrices, 𝐴and 𝐵, with dimensions 𝑁× 𝑑and 𝑑× 𝑁, 29 respectively, where𝑑is a small number relative to𝑁. To estimate the two matrices, the neural matrix factorization 30 method is utilized with its loss composed of the following two parts:

31 • −[ 𝑝𝑡+1𝐺log(𝑜)) + (1− 𝐺) log(1− 𝑜))], where𝑜∈ [0,1] is the linearly weighted output of the graph-based 32 𝑛𝑡+1 𝑡 𝑡 𝑡 𝑡 𝑡

33 MARL𝜇 algorithm.𝑎 2Here, 𝑝𝑡 and 𝑛𝑡serv2 e to alleviate the issue of the seriously imbalanced data.![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.029.png)

34 • 2 Σ𝑖𝑗𝑠𝑖𝑗𝑖− 𝑎𝑗2 + Σ𝑖𝑗𝑠𝑖𝑗𝑏𝑖− 𝑏𝑗2 , where 𝑠𝑖𝑗is the similarity between user 𝑖’s state and user 𝑗’s state, 𝑎𝑖is 35 the 𝑖-th row of the 𝐴matrix, and 𝑏𝑖is the 𝑖-th column of the 𝐵matrix. We use this term to encourage that the

36 more similar between user𝑖’s state and user 𝑗’s state, the more similar between user𝑖’s relationship to others and 37 user 𝑗’s relationship to others.

38 Note that although there might exist a great deal of promising matrix factorization methods [1, 1, 24? ? ] and 39 other algorithms appropriate to update the utility matrix, our MA-DGNN with the above neural matrix factorization 40 mechanism has already achieved remarkable performances in extensive experiments. Here we leave the study of 41 other approaches that optimize the utility matrix as the future work.

42

43 4 DISCUSSION

44 In this section, we illustrate several aspects that need to be noted for our proposed MA-DGNN algorithm.

45

46 • Q: Why do not we adopt the Gumbel-softmax sampling trick for DDPG to conduct discrete actions?

47 A: The difference between our method and the Gumbel-softmax sampling trick is that we compare the 48 aggregated output with coefficientsderived by neural matrix factorization with a sliding-window threshold 49 instead of performing sampling according to each value output by each agent. For our MA-DGNN, if we 50 adopt Gumbel-softmax sampling, then the neural matrix factorization technique could not be implemented 51 due to the lack of the loss derived by the aggregated outputs and thus the credit assignment mechanism

ACM Transactions on Recommender Systems Page 12 of 20

62

62

62

62 Multi-agent Reinforcement Learning for Mention Recommendation • 111:11

5

6 also could not be carried out, which significantlydegrades MA-DGNN’s performance in the experimental 7 evaluation. Therefore, we do not consider the Gumbel-softmax sampling trick.

8 • Q: What are the motivation and advantages of each component in MA-DGNN?

9 A: (1) DGNN: The preferences of each user are dynamic, so are the interactions between users, which are 10 very similar to the phenomenon of population evolution in a complex network. Based on this, we associate 11 the DGNN (delayed aggregation graph neural network) algorithm, which is very suitable for modeling the 12 population evolution of complex networks. DGNN is expected to capture the changes of each individual and 13 of the relationships between individuals adaptively. Also, it is hoped that by using DGNN, the decisions of 14 individuals with sparse interactions can be derived from individuals with richer interaction information.

15 (2) MADDPG: When using a centralized evaluation function, the decision mechanism for each intelligence 16 is influencednot only by the external dynamic environment but also by the changing decision mechanisms 17 for other intelligence, which may make the decision updating process very unstable. In order to make the 18 training process as stable and high-quality as possible, MADDPG assumes that all other intelligence make 19 the worst decisions when optimizing each intelligence and expects the current intelligence to optimize its 20 own decisions to the best under such a worst-case scenario. In summary, MADDPG is expected to help the 21 algorithm keep optimizing in a good quality action space stably, rather than accidentally fall into a bad action 22 space that is hard to get out of.

23 (3) The customized replay buffer: Sample selection in MARL has a significantimpact on the effectiveness 24 of the algorithm. The time of appearance of samples, the positivity or negativity samples, the number of 25 historical appearances of corresponding users, the similarity between users, and the quality of each user’s 26 assisted decision-making for other users are all factors that should be taken into account when performing 27 sample selection. In this regard, we develop a prioritized experience replay mechanism based on six factors 28 to select suitable samples for training. In addition, we also perform sample selection based on state clustering 29 to promote sample diversity. Besides these, we also measure the non-stationarity of the recent environment 30 based on the concentration of the sample pools in the same class and eliminate a certain amount of old 31 samples according to the degree of non-stationarity. The carefully modulated sampling strategy results in 32 real-time, high-quality policy updates, and we can see the clear advantages of this mechanism in ablation 33 learning.

34 (4) Neural matrix factorization: To tackle the credit assignment in MARL, we construct a user-user utility 35 matrix and update it in real-time with a supervised learning algorithm. Due to the sparse information of some 36 user-user interactions, it is difficult to accurately estimate the corresponding elements of the utility matrix 37 using a normal deep neural network. In this regard, we adopt the neural matrix factorization that can cope 38 well with the problem of missing data by learning hidden factors, and develop a loss function for mitigating 39 the positive and negative sample imbalance while exploiting the similarity of user states to train neural matrix 40 factorization, and we can see the great improvement of this technique from the latter-stated ablation study 41 section.

42

44 DANAU-HMNNCoA-CAMN DANAU-HMNNCoA-CAMNNEM 0.875 CoA-CAMNNEM

43 DANAU-HMNN

45 NEMG2ANetMAAC 0.700.750.80 G2ANetMAAC 0.8000.8250.850 MAACCROMA 0.7500.7750.800

G2ANet

46 CROMAOurs 0.600.65 Precision CROMAOurs 0.7000.7250.7500.775 Recall Ours 0.6750.7000.725 F1-Score

0.55 0.675 0.650

47 B C C C

48 1 x 2 A y 0 x 1 B y 0 x 1 B y![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.030.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.031.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.032.png)

3 2 A 2 A

49

50 Fig. 3. Comparison results on the 9 synthetic datasets with K=3.

51

62

62 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56 57 58 59 62

5

6

8 0.80.9 DANAU-HMNNCoA-CAMNNEM 0.850.90 0.70.8 DANAU-HMNNCoA-CAMNNEMG2ANetMAAC

7

9 0.50.60.7 CROMAOurs 0.750.80 DANAU-HMNN 0.6 Ours

G2ANet

MAAC

CROMA

10 0.3 0.65 CoA-CAMNNEMG2ANetMAAC 0.4

0.5

0.4 0.70

11 0.2 0.60 CROMAOurs 0.3

12 1 (a) Number of Recommended Users2 3 4 5 1 (b) Number of Recommended Users2 3 4 5 1 (c) Number of Recommended Users2 3 4 5

13

0.8 0.8

14 0.7 DANAU-HMNN 0.90 DANAU-HMNN DANAU-HMNNCoA-CAMN

16 0.50.6 CoA-CAMNNEMG2ANetMAAC 0.800.85 CoA-CAMNNEMG2ANetMAAC 0.6 NEMG2ANetMAACCROMA

15 0.7

17 0.4 CROMAOurs 0.75 CROMAOurs 0.5 Ours

18 0.3 0.70 0.4

19 0.2 0.600.65 0.3

20 1 (d) Number of Recommended Users2 3 4 5 1 (e) Number of Recommended Users2 3 4 5 1 (f) Number of Recommended Users2

3 4 5

21 Fig. 4. Precision, recall, and F1-Score in different number of recommended users. (a)(b)(c): performances on the 22 Twitter dataset; (d)(e)(f): performances on the Sina Weibo dataset. Algorithms are run for 3 times.

23

24 Table 2. Overall performance on the Twitter dataset. We repeat each experiment 3 times and report the average

25 scores and standard errors (displayed in the brackets). The best results are given in bold. The higher the metric, the 26 better performance of the model.

27 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.033.png)

28 Twitter

Model

29 Prec. Recall F1. MRR Hits@3 Hits@5

30 DAN 0.7237 (0.0441) 0.6231 (0.0182) 0.6696 (0.0319) 0.6402 (0.0284) 0.7103 (0.0298) 0.6287 (0.0281) 31 AU-HMNN 0.7482 (0.0458) 0.6721 (0.0197) 0.7081 (0.0341) 0.6948 (0.0311) 0.7354 (0.0339 ) 0.6842 (0.0302) 32 CoA-CAMN 0.7383 (0.402) 0.6449 (0.0193) 0.6893 (0.0327) 0.6695 (0.0259) 0.7346 (0.0291) 0.6541 (0.0251) 33 NEM 0.7314 (0.0386) 0.6756 (0.0207) 0.7024 (0.0305) 0.6894 (0.0298) 0.7233 (0.0327) 0.6803 (0.0247) 34 G2ANet 0.6533 (0.0442) 0.6539 (0.0248) 0.6284 (0.0327) 0.6795 (0.0334) 0.6406 (0.0356) 0.6665 (0.0292)

MAAC 0.6781 (0.0476) 0.6376 (0.0297) 6689 (0.0385) 0.6548 (0.0304) 0.6735 (0.0374) 0.6461 (0.0312) 35 CROMA 0.7080 (0.0437) 0.6621 (0.0228) 0.6974 (0.0336) 0.6832 (0.0314) 0.7027 (0.0355) 0.6725 (0.0261)

36 MA-DGNN 0.8862 (0.0397) 0.7464 (0.0209) 0.8103 (0.0318) 0.7749 (0.0294) 0.8179 (0.0308) 0.7592 (0.0254) 37

38 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.034.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.035.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.036.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.037.png)![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.038.png)

39 

41 0.800.810.82 0.7750.8000.825 I\_1I\_2I\_3I\_4I\_5 0.840.860.880.90 PrecisionRecallF1-Score

42 0.760.77 0.7250.750 0.740.760.780.80 0.82 0.800.820.84

40

0.79 I\_6 0.78

43 0.75 I\_1I\_2I\_3I\_4I\_5 0.6750.700 0.700.72 I\_1I\_2I\_3I\_4I\_5I\_6 0.760.780.80 PrecisionRecall 0.760.78

I\_6 F1-Score

44 0.0 0.5 1.0 1.5 (a) 2.0n 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 (b) 2.0n 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 (c) 2.0n 2.5 3.0 3.5 4.0 0.2 (d)  0.4: coefficient of the similarity-based loss1.0 2.0 4.0 10.0 20.0 10 20 (e) number of clustering

30 40 60 80 100

45 Fig. 5. Hyper-parameter analysis on the Twitter dataset. (a)(b)(c): analysis of six indexes in replay buffer; (d) analysis 46 of 𝜇, the coefficient of the similarity-based loss in neural matrix factorization; (e) analysis of the number of sample 47 clusters in replay buffer.

48

49 5 EXPERIMENTS

50 In this section, we conduct extensive experiments on both the real-world and synthetic datasets to answer the 51 following questions:

ACM Transactions on Recommender Systems Page 14 of 20

62

62

62

62 Multi-agent Reinforcement Learning for Mention Recommendation • 111:13

5

6 Table 3. Overall performance on the Sina Weibo dataset.

7 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.039.png)

8 Sina Weibo

Model

9 Prec. Recall F1. MRR Hits@3 Hits@5

10 DAN 0.7749 (0.0498) 0.6502 (0.0312) 0.7071 (0.0428) 0.6443 (0.0391) 0.8341 (0.0418) 0.6551 (0.0301) 11 AU-HMNN 0.8012 (0.0547) 0.7002 (0.0384) 0.7473 (0.0458) 0.6975 (0.0429) 0.9116 (0.0428) 0.7147 (0.0312) 12 CoA-CAMN 0.7881 (0.0479) 0.6751 (0.0301) 0.7272 (0.0392) 0.6702 (0.0433) 0.8703 (0.0402) 0.6829 (0.0274) 13 NEM 0.7803 (0.0519) 0.6875 (0.0349) 0.7310 (0.0472) 0.6987 (0.0445) 0.8291 (0.0462) 0.7263 (0.0329) 14 G2ANet 0.7329 (0.0581) 0.6705 (0.0362) 0.7835 (0.0481) 0.6806 (0.0468) 0.83.49 (0.0479) 0.7025 (0.0337)

MAAC 0.7532 (0.0558) 0.6634 (0.0364) 0.80.02 (0.0477) 0.6720 (0.0459) 0.8627 (0.0481) 0.6973 (0.0309) 15 CROMA 0.7838 (0.0476) 0.6728 (0.0317) 0.8269 (0.0401) 0.6809 (0.0424) 0.8783 (0.0417) 0.7023 (0.0316)

16 MA-DGNN 0.8879 (0.0513) 0.7853 (0.0313) 0.8335 (0.0418) 0.7851 (0.0421) 0.9341 (0.0441) 0.8195 (0.0308) 17

18

19 Table 4. Overall performance on the Twitter dataset for users with sparse interactions.

20 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.040.png)

21 Model Twitter

22 Prec. Recall F1. MRR Hits@3 Hits@5

23 LambdaMart 0.5218 (0.0435) 0.5031 (0.0385) 0.5122 (0.0402) 0.4894 (0.0463) 0.6649 (0.0441) 0.4984 (0.0369) 24 DAN 0.4828 (0.0387) 0.4595 (0.0394) 0.4708 (0.0389) 0.4398 (0.0384) 0.6702 (0.0405) 0.4452 (0.0406) 25 AU-HMNN 0.5084 (0.0401) 0.4792 (0.0413) 0.4933 (0.0409) 0.4423 (0.0428) 0.6996 (0.0416) 0.46080.4378 (0.0387)(0.0391)

CoA-CAMN 0.4934 (0.0426) 0.4813 (0.0374) 0.4872 (0.0399) 0.4294 (0.0393) 0.6603 (0.0385)

26 NEM 0.4915 (0.0387) 0.4736 (0.0415) 0.4824 (0.0401) 0.4401 (0.0418) 0.6302 (0.0406) 0.4484 (0.0375) 27 G2ANet 0.4618 (0.0372) 0.4495 (0.0384) 0.4556 (0.0379) 0.4375 (0.0375) 0.6297 (0.0394) 0.4457 (0.0417) 28 MAAC 0.4814 (0.0421) 0.4602 (0.0398) 0.4706 (0.0403) 0.4473 (0.0369) 0.6583 (0.0371) 0.4552 (0.0433) 29 CROMA 0.5097 (0.0386) 0.4831 (0.0416) 0.4960 (0.0396) 0.4571 (0.0414) 0.6589 (0.0428) 0.4638 (0.0396) 30 Ours 0.6618 (0.0378) 0.6487 (0.0409) 0.6552 (0.0392) 0.5725 (0.0391) 0.7495 (0.0397) 0.5784 (0.0387) 31

32 Table 5. Overall performance on the Sina Weibo dataset for users with sparse interactions.

33 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.041.png)

34 Model Sina Weibo

35 Prec. Recall F1. MRR Hits@3 Hits@5

36 LambdaMart 0.5572 (0.0375) 0.5381 (0.0391) 0.5475 (0.0389) 0.4891 (0.0385) 0.7281 (0.0445) 0.5084 (0.0397) 37 DAN 0.5193 (0.0435) 0.4794 (0.0398) 0.4986 (0.0417) 0.4615 (0.0391) 0.6983 (0.0375) 0.4672 (0.0381) 38 AU-HMNN 0.5382 (0.0394) 0.4823 (0.0371) 0.5087 (0.0384) 0.4732 (0.0402) 0.7292 (0.0381) 0.4823 (0.0418) 39 CoA-CAMN 0.5294 (0.0403) 0.4685 (0.0348) 0.4971 (0.0381) 0.4489 (0.0375) 0.6827 (0.0416) 0.4509 (0.0394) NEM 0.5271 (0.0391) 0.4703 (0.0361) 0.4971 (0.0386) 0.4632 (0.0329) 0.6494 (0.0427) 0.4639 (0.0402)

40 G2ANet 0.4895 (0.0324) 0.4617 (0.0413) 0.4583 (0.0423) 0.6582 (0.0395) 0.4595 (0.0379) 41 MAAC 0.4931 (0.0457) 0.4763 (0.0381) 0.4752 (0.0409) 0.4672 (0.0374) 0.6802 (0.0418) 0.5387 (0.0342)

0.4846 (0.0421)

42 CROMA 0.5374 (0.0413) 0.4894 (0.0362) 0.5123 (0.0392) 0.4826 (0.0387) 0.6855 (0.0373) 0.5491 (0.0371) 43 Ours 0.6707 (0.0394) 0.6182 (0.0328) 0.6434 (0.0371) 0.6094 (0.0342) 0.7507 (0.0379) 0.6561 (0.0421) 44

45

46 RQ1 How does MA-DGNN perform on various datasets for mention recommendation, compared with existing 47 baselines?

48 RQ2 How do different configurationsof hyper-parameters and different indexes in the prioritized replay buffer

49 influencethe overall performance of our proposed algorithm?

50 RQ3 What are the influences of each component in MA-DGNN, such as the credit assignment procedure, the

51 mimimax optimization of the critic losses, etc.?

62

62 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56

57

58

59 62

111:14 • Hanchi Huang, Yan Li, Li Shen, Jianghao Lin, and Xin Xin

4

5

Table 6. Ablation study on the Twitter dataset.

6

7

|Metric|Dataset|MA-DGNN|𝑡ℎ𝑟�𝑡= 0.5|�w/o CER|w/o SBL|w/o CRA|w/o MMO|
| - | - | - | - | - | - | - | - |
|F1-Score|Twitter Sina|0.8103 0.8335|0.6708 0.7129|0.7506 0.7439|0.7834 0.7902|0.7681 0.7792|0.7792 0.7875|
|Hit@3|Twitter Sina|0.8179 0.9341|0.6951 0.8017|0.7428 0.8517|0.7826 0.9021|0.7606 0.8693|0.7693 0.8838|
|Hit@5|Twitter Sina|0.6803 0.8195|0.6175 0.6982|0.6189 0.7473|0.6695 0.7928|0.6417 0.7686|0.6579 0.7802|
8

9

10

11

12

13

14

15

16

5.1 Experiment Setup

17

1. Dataset. We evaluate our proposed MA-DGNN and the selected baseline models on both the real-world and synthetic datasets.

18

19

Real world datasets. We adopt two real datasets, the Twitter public dataset [? ] and the crawled Sina Weibo dataset7, from the two most popular social network platforms, Twitter and Sina Weibo. The statistics of the

20

21

real-world datasets are shown in Table 7.

22

23

Table 7. The dataset statistics.

24

25

||Twitter|Sina Weibo|Synthetic|
| :- | - | - | - |
|#Twitters / #Weibo|200,465|56,673|60,000|
|#Images|200,465|60,652|0|
|#Users|15,539|2,000|2,000|
|#Avg. Mention per User|39.45|36.23|41.77|
|#Avg. Mentioned per User|7.51|6.97|10.32|
26

27

28

29

30

31

Synthetic datasets. As for the construction of synthetic datasets, for 𝑖∈ [𝑁] and 𝑡∈ [𝑇], we set the feature vector of user 𝑖at time 𝑡to be: 𝜃= (𝑐𝑜𝑠(𝑢𝑡),···,𝑐𝑜𝑠(𝑢𝑡))~~ , where𝑑is the dimension of the feature vectors and

32

𝑖,𝑡||(𝑐𝑜𝑠(𝑢𝑖𝑖,1 ,1𝑡),···,𝑐𝑜𝑠(𝑢𝑖𝑖,𝑑,𝑑𝑡))||

33

{𝑢|𝑖∈ [𝑁], 𝑗∈ [𝑑]}are the constant coefficientsin different2 components.

34

𝑖,𝑗

For each 𝑡∈ [𝑇], we randomly generate a tweet 𝑡𝑤𝑡for the currently served user 𝑖. The feature vector of 𝑡𝑤𝑡is set to be 𝑣𝑡, where 𝑣𝑡,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0,1]) for ∀𝑗∈ [𝑑]. At this time, the user𝑡mentioned by user 𝑖𝑡is

35

36 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.042.png) 37

argmax𝑗∈[𝑁] 𝜃⊤||𝑡𝑣 𝑤||𝑡− ||𝜃𝑗,𝑡− 𝜃𝑖𝑡,𝑡||2, i.e., the more similar to user 𝑖a user is and the larger projection length on 𝑗,𝑡𝑡 𝑡

38

the vector 𝑡𝑤𝑡a user owns, the more likely he/she is to be mentioned by user 𝑖𝑡at time 𝑡.

39

To demonstrate the robustness of our proposed MA-DGNN, we construct synthetic datasets with different degrees of non-stationarity and different distributions of users’ appearing frequency. Specifically, with regard to users’ varying preferences, i.e., the non-stationarity, we classify the synthetic datasets into 3 categories:

40

41

42

(𝑏) :𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 𝐼𝐼: 𝑓𝑜𝑟 𝑖∈ [𝑁], 𝑗∈ [𝑑],𝑢𝑖,𝑖𝑗,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0, 320𝑇]); (𝑎) :𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 𝐼: 𝑓𝑜𝑟 𝑖∈ [𝑁], 𝑗∈ [𝑑],𝑢∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0, ]);

10

43

 3𝑇

44

45

 10

46

(𝑐) :𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 𝐼𝐼𝐼: 𝑓𝑜𝑟 𝑖∈ [𝑁], 𝑗∈ [𝑑],𝑢𝑖,𝑗∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚([0, ]).

47

- 𝑇

48

Regarding users’ appearing frequency, we also classify the synthetic datasets into3 categories: (a) category A: at each time𝑡∈ [𝑇], each user is uniformly randomly served, i.e., each user issues a tweet with the same probability

49

50 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.043.png)

7We crawl the Sina Weibo according to [? ] and process the crawled dataset through the same procedures as the literature [21]. Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022.

51

52

53

ACM Transactions on Recommender Systems Page 16 of 20

61

61

61

61 Multi-agent Reinforcement Learning for Mention Recommendation • 111:15

5

6 at each round; (b) category B: users are served in a totally randomized way. (c) category C: we randomly select10 7 percent of users to issue approximately 60percent of tweets. And in the rest rounds, the rest users are served in a 8 totally randomized way.

9 We cross-link categories I, II, III with categories A, B, C and thus construct 9 (= 3× 3) synthetic datasets.

10

2. Baselines. The existing mention recommendation models can be categorized into two groups: the super-

11

vised learning-based (SL-based) group and the multi-agent reinforcement learning-based (MARL-based) group. As 12

13 shown in Table 2,3, we consider DAN [22], AU-HMNN [13], CoA-CAMN [218] and NEM [39] as representative

SL-based baselines, and incorporate each of them with the actor-critic method for fair comparison with our rein- 14

forcement learning approach. For MARL-based methods, we choose G2ANet [20], MAAC [15] and CROMA [9] 15

as baselines. Below, we describe our selected baselines in brief:

16

1) DAN. Dual-attention (DAN) is an attention method introduced in [22] which models the relationship between

17

objects from different perspectives.

18

2) AU-HMNN. AU-HMNN is proposed in [13], which incorporates the textual information of query tweets and

19

20 user(iii)histories.CoA-CAMN.This is aTostate-of-the-artuse textual andapproachvisual information,to the mentionRenfengrecommendationet al. [21] proposetask. a novel cross-attention 21

22 memory(iv) NEM.netwYorki ettoal.perform[39] constructthe mentiona heterogeneousrecommendationmentiontasknetwfor orkmultimodalaccordingtweets.to different relationships among 23

24 difstructureferent entities,and verteandx contentthen inferinformationa unifiedlointow dimensionalaccount. embedding for users and messages by taking the network 25 (v) G2ANet. Yong et al. [20] model the relationship between agents by a complete graph and propose a

26 novel game abstraction mechanism based on the integration of a two-stage attention network (G2ANet) with the 27

28 multi-agent(vi) MAAreinforcementC. MAAC [15learning] is an actortraining-criticmechanism.algorithm that trains decentralized policies in multi-agent settings, 29 using centrally computed critics that share an attention mechanism which selects relevant information for each agent

30 at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent 31 environments when compared to recent approaches.

32 (vii) CROMA. CROMA [9] is a novel cooperative multi-agent approach to mention recommendation, which 33 incorporates dozens of more historical tweets than earlier approaches.

34 (viii) LambdaMart. LambdaMart [2] is a Listwise-learning-to-rank algorithm that transforms the ranking 35 problem into a regression decision tree problem. As a classic learning-to-rank method, it is still widely applied in 36 the search engine and recommendation systems of major Internet companies.

37

38 5.1.3 Evaluation Metrics. Following previous works [9, 20, 39], we take the Precision, Recall, F1-score [29], 39 and Mean Reciprocal Rank (MRR) [25] as evaluation metrics for the highest-ranked result. Furthermore, Hits@3 40 and Hits@5 [13] are reported to denote the percentage of correct results recommended from the top-K results. 41 Higher values of these metrics indicate better performance of the mention recommendation model.

42 5.1.4 Implementation Details. In this work, we set the discounted factor 𝛾of MDP to be 0.99, the length of 43

44 𝑆𝑒𝑞𝐶𝑜𝑛𝑣’s input 𝑁𝑙𝑖𝑠𝑡to be 10, the perturbations𝛼𝑖(𝑖∈ [𝑁]) during minimax optimization to be10−5, the number

of hops in the decentralized actors to be 2, and the number of sample clusters in the customized replay buffer to 45 be 20. The coefficient of the similarity-based loss (i.e., 𝜇) in the neural matrix factorization component is set to

46 be 0.4. Moreover, the learning rates for the actor network, the critic network, and the neural matrix factorization 47 mechanism are set to be 10−5, 10−4, and 10−4, respectively. Our model implemented on PyTorch is available for 48 reviewers9 and will be publicly available upon the acceptance of this work.

49 ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.044.png)

50 8We take the supervised method as the policy network and the critic network of actor-critic.

51 9Code: https://filedropper.com/d/s/dLEu015PoqARaoZOWSIgu9J3LKYbXL .

61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56

57

58 59 61

4 111:16 • Hanchi Huang, Yan Li, Li Shen, Jianghao Lin, and Xin Xin

5

6 5.2 Overall Performance (RQ1)

7 We firstperform each mention recommendation model on two real-world datasets, i.e., Twitter and Sina Weibo. 8 The results are presented in Table 2,3, and Figure 4, from which we can obtain the following observations:

9 (i) MA-DGNN significantly outperforms all the baseline models, especially for the Precision metric. Such 10 improvement validates the effectiveness of applying MARL with the delayed aggregation graph neural network, 11 which ensures the personalized recommendation for each target user and the in-time adjustment for users’ dynamic 12 preferences.

13 (ii) As for the performance analysis of other baselines, SL-based methods generally outweigh MARL-based 14 methods except MA-DGNN, which forms a vivid contrast to the superior performance of MA-DGNN and consis- 15 tently demonstrates the effectiveness of our proposed components (DGNN, the customized priority replay buffer, 16 and the minimax optimization technique) in MA-DGNN to improve over traditional MARL methods. Therefore, 17 MA-DGNN enables a better MARL-policy learning process and gains significantperformance improvement.

18 To further evaluate MA-DGNN and baselines under different degrees of non-stationarity and user-appearing 19 frequency, we construct 9 synthetic datasets and report the Precision, Recall and F1-Score for each model. The 20 results are shown in Figure 3, where our MA-DGNN shows absolute competence against other baselines in all 9 21 synthetic datasets, especially when the user appearance is non-uniform and the environment is highly non-stationary. 22 In Figure4, we show the Precision, Recall, and F1-score of different algorithms with the number of recommended 23 users ranging from one to five. From Figure 4, the performances of MA-DGNN rank the highest regarding all 24 the three metrics in all real datasets. Especially for the recall metric, however we change the number of users 25 between one and five, the average recall rate of our MA-DGNN is consistently above 0.75, which shows absolute 26 competence compared with other baseline methods.

27

28 5.3 Sparse Users Analysis

29 To test the advantage of dealing with sparse users’ interactions for MA-DGNN, on the Twitter and Sina Weibo 30 datasets, we only select users whose Ag. Mention times is less than 20 percent of the mean times or users whose Ag. 31 Mentioned times is less than 20 percent of the mean times. We train MA-DGNN and all baselines on the selected 32 users and the performances are shown in Table4 and 5. As can be seen in Table4 and 5, our MA-DGNN still beats 33 all other baselines whether on the Twitter dataset or the Sina Weibo dataset, which shows the strong capability of 34 MA-DGNN to tackle with users with less interaction information.

35

36 5.4 Hyper-parameter analysis (RQ2)

37 In this section, we test the influenceof different configurationsof hyper-parameters on the overall performance in 38 the Twitter dataset. Recall that 𝜇is the coefficient of the similarity-based loss in our neural matrix factorization 39 module and we define𝑐𝑛to be the number of sample clustering in the replay buffer.

40 (i) Set𝑐𝑛= 20and 𝜇= 0.2,0.4,1,2,4,10,20. Results under different values of𝜇are shown in Figure 5 (d).

41 (ii) Set 𝜇= 0.4and𝑐𝑛= 10,20,30,40,60,80,100. Results under different values of𝑐𝑛are shown in Figure5 (e)10. 42 From Figure 5 (d)(e), we can see that the overall performances of MA-DGNN are positively correlated to the 43 coefficientof the similarity-based loss and applying an appropriate number of sample clusters in the replay buffer 44 also improves MA-DGNN significantly

45 Next, we study the influenceof different indexes in the prioritized replay buffer on the overall performance in 46 detail. For each 𝑚∈ [6] and each 𝑛∈ [6], we firstfix𝑚and 𝑛, and then set the sampling probability for the 𝑖’s 47 sample to be 𝑗=1𝑧𝑖,𝑗+(𝑛−1)𝑧𝑖,𝑚. By varying 𝑚∈ [6] and 𝑛∈ [6], we can obtain the performances of our proposed

6

48 6

49 model under different proportions of the six indexes in the replay buffer (See Figure 5 (a)(b)(c) for the Twitter ![](Aspose.Words.73e43c4f-ed2a-4f9f-b0ff-92094f7e0fb0.045.png)50 10Although MA-DGNN is somewhat sensitive to𝑐𝑛, it has consistently great performances which outweigh the second best method in a wide

51 range of𝑐𝑛in the Twitter dataset. Therefore, we do not need to overly worry about the setting of𝑐𝑛.

52

53 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022.

ACM Transactions on Recommender Systems Page 18 of 20

61

61

61

61 Multi-agent Reinforcement Learning for Mention Recommendation • 111:17

5

6 dataset and Figure 5 (a)(b)(c) for the Sina Weibo dataset with the number of recommended user set to be1). From 7 Figure 5, we can see that increasing the proportions of the indexes (ii)(v)(vi) could contribute to improving the 8 performance regarding the recall and F1-score metrics. The performance with regard to Precision also does not 9 hurt much when we pay more attention to indexes (ii)(v)(vi). These demonstrate the importance of whether the 10 user feedback is positive or negative, the averaged performance that a user assists on other users’ decisions, and 11 the averaged similarity between a user’s state and other users’ states in boosting the prioritized experience replay 12 mechanism.

13

14 5.5 Ablation study (RQ3)

15

16 For the ablation study, we test our method when 𝑡ℎ𝑟𝑒𝑡= 0.5 and in the lack of the customized experience replay

(CER) mechanism, the similarity-based loss in neural matrix factorization (SBL), the credit assignment procedure 17 (CRA), and the mimimax optimization of the critic losses (MMO), separately, on both the Twitter dataset and

18 𝑛+1 19 the Sina Weibo dataset. As shown in Table 6, no matter for F1-score, hits@3, or hits@5, replacing 𝑡ℎ𝑟𝑒𝑡= 𝑝𝑡𝑡+1

20 with 𝑡ℎ𝑟𝑒𝑡= 0.5 and the lack of any component bring varying degrees of reduction in algorithm performance, 21 which consistently demonstrates the Precision of algorithm design and the importance of tacit𝑛+1 cooperation between 22 different components. The obvious performance deterioration when replacing 𝑡ℎ𝑟𝑒𝑡= 𝑝𝑡+1 with 𝑡ℎ𝑟𝑒𝑡= 0.5 and

23 when lacking the customized experience replay component specifically show the effectiveness𝑡 and efficiency of our 24 design of 𝑡ℎ𝑟𝑒𝑡, and our selected six indexes and the non-stationary detection mechanism with state clustering in

25 the replay buffer.

26

27 6 CONLUSION

28 Mention recommendation is an important tool in social networking platforms to boost users’ experience. To deal 29 with the dynamic and sparse interactions between users which are seldom considered by existing literature, we 30 apply multi-agent reinforcement learning with customized prioritized experience replay to model users’ varying 31 relationship and employ a delayed aggregation graph neural network as the decentralized actor. A user-user 32 utility matrix optimized by neural matrix factorization with a similarity-based loss is designed to help make a 33 comprehensive decision and perform credit assignment. In the experimental evaluation, apart from two real-world 34 datasets, we also construct various synthetic datasets with different degrees of non-stationarity and distributions of 35 users’ appearing frequency. Extensive experiments demonstrate the effectiveness of our design in characterizing 36 users’ relationship and our method which has expressive advantages over existing methods. In the future, we plan 37 to improve the explainability of MA-DGNN and its application in more specificscenarios, such as scenarios where 38 different users’ social circles are extremely imbalanced.

39

40 REFERENCES

41

42 [1] InformationSanjeev Arora,ProcessingNadav Cohen,SystemsW32ei Hu,(2019),and 7413–7424.Yuping Luo. 2019. Implicit regularization in deep matrix factorization. Advances in Neural 43 [2] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.

44 [30] ]crawl Lei Chen. [n.d.]. A tool for crawling Sina Weibo. [https://github.com/dataabc/weiboSpider.](https://github.com/dataabc/weiboSpider)

45 [4] Kevin Dabérius, Elvin Granat, and Patrik Karlsson. 2019. Deep Execution-Value and Policy Based Reinforcement Learning for Trading 46 and Beating Market Benchmarks. Available at SSRN 3374766(2019).

47 [5] Zhaoappropriateyun Ding,timeXueqingon TwitterZou,. InYAsia-PueyangacificLi, SuWebHe,ConferJiajunenceCheng,. SpringerFengcai, 464–468.Qiao, and Hui Wang. 2016. Mentioning the optimal users in the 48 [6] Laura Fontanesi, Sebastian Gluth, Mikhail S Spektor, and Jörg Rieskamp. 2019. A reinforcement learning diffusion decision model for

49 value-based decisions. Psychonomic bulletin & review26, 4 (2019), 1099–1121.

50 [7] Fernando Gama, Antonio G Marques, Alejandro Ribeiro, and Geert Leus. 2019. Aggregation graph neural networks. InICASSP 2019-2019 51 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 4943–4947.

61

113 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

55

56

57

58

59 61

4 111:18 • Hanchi Huang, Yan Li, Li Shen, Jianghao Lin, and Xin Xin

5

6 [30] ]githubcroma Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua Zhou, and Xuanjing Huang. [n.d.]. Pytorch implementation 7 of Mention Recommendation in Twitter with Cooperative Multi-Agent Reinforcement Learning. [htps://github.com/mritma/CROMA.](htps://github.com/mritma/CROMA)

8 [9] Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua Zhou, and Xuanjing Huang. 2019. Mention recommendation in Twitter 9 with cooperative multi-agent reinforcement learning. In Proceedings of the 42nd International ACM SIGIR Conference on Research and

Development in Information Retrieval. 535–544.

10 [10] Taehyun Ha, Seunghee Han, Sangwon Lee, and Jang Hyun Kim. 2017. Reciprocal nature of social capital in Facebook: an analysis of 11 tagging activity. Online Inf. Rev.41, 6 (2017), 826–839.

12 [11] Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, and Zhan Chen. 2018. Playing 20 question game with 13 policy-based reinforcement learning. arXiv preprint arXiv:1808.07645 (2018).

14 [12] Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu. 2018. Reinforcement learning to rank in e-commerce search engine:

Formalization, analysis, and application. InProceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & 15 Data Mining. 368–377.

16 [13] Haoran Huang, Qi Zhang, Xuanjing Huang, Haoran Huang, Qi Zhang, and Xuanjing Huang. 2017. Mention Recommendation for Twitter 17 with End-to-end Memory Network.. In IJCAI. 1872–1878.

18 [14] Jeff Huang, Katherine Thornton, and Efthimis N. Efthimiadis. 2010. Conversational tagging in twitter. InHT’10, Proceedings of the 21st 19 ACM Conference on Hypertext and Hypermedia, Toronto, Ontario, Canada, June 13-16, 2010, Mark H. Chignell and Elaine G. Toms

(Eds.). ACM, 173–178.

20 [15] Shariq Iqbal and Fei Sha. 2019. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine 21 Learning. PMLR, 2961–2970.

22 [16] Bo Jiang, Ying Sha, and Lihong Wang. 2015. Predicting user mention behavior in social networks. InNatural Language Processing and 23 Chinese Computing. Springer, 146–158.

[17] Quanle Li, Dandan Song, Lejian Liao, and Li Liu. 2015. Personalized mention probabilistic ranking–recommendation on mention behavior

24 of heterogeneous social network. In International conference on web-age information management. Springer, 41–52.

25 [18] Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. 2019. Robust multi-agent reinforcement learning via minimax 26 deep deterministic policy gradient. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 33. 4213–4220.

27 [19] Feng Liu, Ruiming Tang, Xutao Li, Yunming Ye, Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2018. Deep Reinforcement 28 Learning based Recommendation with Explicit User-Item Interactions Modeling. CoRRabs/1810.12027 (2018). arXiv:[1810.12027](https://arxiv.org/abs/1810.12027)

<http://arxiv.org/abs/1810.12027>

29 [20] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. 2020. Multi-agent game abstraction via graph attention 30 neural network. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 34. 7211–7218.

31 [21] Renfeng Ma, Qi Zhang, Jiawen Wang, Lizhen Cui, and Xuanjing Huang. 2018. Mention recommendation for multimodal microblog with 32 cross-attention memory network. InThe 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 33 195–204.

[22] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. 2017. Dual attention networks for multimodal reasoning and matching. InProceedings

34 of the IEEE conference on computer vision and pattern recognition. 299–307.

35 [23] Soumajit Pramanik, Mohit Sharma, Maximilien Danisch, Qinna Wang, Jean-Loup Guillaume, and Bivas Mitra. 2019. Easy-Mention: a 36 model-driven mention recommendation heuristic to boost your tweet popularity.International Journal of Data Science and Analytics 7, 2 37 (2019), 131–147.

38 [24] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018. Network embedding as matrix factorization: Unifying

deepwalk, line, pte, and node2vec. InProceedings of the eleventh ACM international conference on web search and data mining. 459–467. 39 [25] Dragomir R Radev, Hong Qi, Harris Wu, and Weiguo Fan. 2002. Evaluating Web-based Question Answering Systems.. InLREC. Citeseer.

40 [26] Muhammad Rehan Raza, Carlos Natalino, Peter Öhlen, Lena Wosinska, and Paolo Monti. 2018. A slice admission policy based on 41 reinforcement learning for a 5G flexible RAN. In 2018 European Conference on Optical Communication (ECOC). IEEE, 1–3.

42 [27] Saiph Savage, Andrés Monroy-Hernández, Kasturi Bhattacharjee, and Tobias Höllerer. 2015. Tag Me Maybe: Perceptions of Public 43 Targeted Sharing on Facebook. CoRRabs/1509.01095 (2015). arXiv:[1509.01095](https://arxiv.org/abs/1509.01095)

[28] Mohit Sewak. 2019. Policy-Based Reinforcement Learning Approaches. In Deep Reinforcement Learning. Springer, 127–140.

44 [29] Guy Shani and Asela Gunawardana. 2011. Evaluating recommendation systems. InRecommender systems handbook. Springer, 257–297. 45 [30] ]seqconv Shobrook. [n.d.]. SeqConv, a PyTorch implementation of a LSTM-based graph convolutional operator.[ https://pypi.org/project/ ](https://pypi.org/project/seq-conv/)46 [seq-conv/.](https://pypi.org/project/seq-conv/)

47 [31] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.

48 [32] Liyang Tang, Zhiwei Ni, Hui Xiong, and Hengshu Zhu. 2015. Locating targets through mention in Twitter.World Wide Web18, 4 (2015),

1019–1049.

49 [33] Ekaterina Tolstaya, Fernando Gama, James Paulos, George Pappas, Vijay Kumar, and Alejandro Ribeiro. 2020. Learning decentralized 50 controllers for robot swarms with graph neural networks. In Conference on Robot Learning. PMLR, 671–682.

51

52

53 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022.

ACM Transactions on Recommender Systems Page 20 of 20

61

61

61

61 Multi-agent Reinforcement Learning for Mention Recommendation • 111:19

5

6 [34] Beidou Wang, Can Wang, Jiajun Bu, Chun Chen, Wei Vivian Zhang, Deng Cai, and Xiaofei He. 2013. Whom to mention: expand the 7 diffusion of tweets by@ recommendation on micro-blogging systems. InProceedings of the 22nd international conference on World Wide

8 Web. 1331–1340.

9 [35] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD

international conference on Knowledge discovery and data mining. 1225–1234.

10 [36] Kai Wang, Weiyi Meng, Shijun Li, and Sha Yang. 2019. Multi-Modal Mention Topic Model for mentionee recommendation. Neurocom- 11 puting 325 (2019), 190–199.

12 [37] Xuesong Wang, Yang Gu, Yuhu Cheng, Aiping Liu, and CL Philip Chen. 2019. Approximate policy-based accelerated deep reinforcement 13 learning. IEEE transactions on neural networks and learning systems 31, 6 (2019), 1820–1830.

14 [38] Tengyu Xu and Yingbin Liang. 2021. Sample complexity bounds for two timescale value-based reinforcement learning algorithms. In

International Conference on ArtificialIntelligence and Statistics. PMLR, 811–819.

15 [39] Feng Yi, Bo Jiang, and Jianjun Wu. 2020. Heterogeneous information network embedding for mention recommendation. IEEE Access8 16 (2020), 91394–91404.

17 [40] Mengran Yu and Shiliang Sun. 2020. Policy-based reinforcement learning for time series anomaly detection.Engineering Applications of 18 ArtificialIntelligence95 (2020), 103919.

19 [41] Xinshifor trafficZang,signalHuaxiucontrol.Yao,InGuanjieProceedingsZheng,ofNanthe AAAIXu, KaiConferXu, enceand Zhenhuion ArtificialLi. 2020.IntelligMetalight:ence, Vol.V34.alue-based1153–1160.meta-reinforcement learning 20 [42] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise

21 recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems. 95–103.

22 [43] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with negative feedback via 23 pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &

Data Mining. 1040–1048.

24 [44] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep 25 reinforcement learning framework for news recommendation. In Proceedings of the 2018 World Wide Web Conference. 167–176.

26 [45] Ge Zhou, Lu Yu, Chu-Xu Zhang, Chuang Liu, Zi-Ke Zhang, and Jianlin Zhang. 2015. A novel approach for generating personalized 27 mention list on micro-blogging system. In 2015 IEEE international conference on data mining workshop (ICDMW). IEEE, 1368–1374.

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42 43 44 45 46 47 48 49 50 51
61

61 Proc. ACM Meas. Anal. Comput. Syst., Vol. 37, No. 4, Article 111. Publication date: August 2022. 54

113

56

57

58

59

61
